# 概率论与数理统计笔记

# 第一章 导论

&emsp;&emsp;**统计学习（statistical learning）**是一套以理解数据为目的庞大工具集，可分为**监督式（supervised）**学习和**非监督式（unsupervised）**学习。

# 第二章 统计学习

## 2.1 相关概念

1. 统计学习是关于估计$f(\cdot)$的一系列方法，其中$f(\cdot)$为一个定量的响应变量$Y$和$p$个不同的预测变量$X=(X_1,X_2,...,X_p)$之间的关系，一般形式如下：
   $$
   Y=f(X)+\epsilon
   $$
   其中，$\epsilon$是随机误差项（error term），与$X$独立，且均值为0.

   误差项包含了一下因素：
   
   * 真实的关系可能不是$f(\cdot)$，例如在简单线性回归估计中，实际关系可能并不是线性的；
   * 可能是其他变量导致了$Y$的变化；
   * 可能存在测量误差。
   
2. 估计$f(\cdot)$的<u>主要原因</u>可分为**预测（prediction）**和**推断（inference）**，其中：

   * **预测**

     关注预测的结果，不关注模型的可解释性和变量之间的关系，可表示为：
     $$
     \hat{Y}=\hat{f}(X)
     $$
     其中$\hat{Y}$表示$Y$的预测值，依赖于两个量，**可约误差（reducible error）**和**不可约误差（irreducible error）**，可约误差可通过改进统计学习方法降低，而不可约误差$\epsilon$是无法降低的，所以即使得到一个$f$的精确估计，预测仍然存在误差，预测的均方误差可表示为
     $$
     \begin{align}
     E(Y-\hat{Y})^2=&E[f(X)+\epsilon-\hat{f}(X)]^2\\
     =&\underbrace{[f(X)-\hat{f}(X)]^2}_{可约误差}+\underbrace{Var(\epsilon)}_{不可约误差}
     \end{align}
     $$
     
* **推断**
  
  目标不是为了预测$Y$，而是想明白$X$和$Y$之间的关系，可以描述为以下问题：
  
  * 哪些预测变量与响应变量相关？
     * 响应变量与每个预测因子之间的关系是什么？
     * $Y$与每个预测变量的关系是否能用一个线性方程概括，还是需要更复杂的形式？
  
3. 估计$f(\cdot)$的<u>方法</u>可分为**参数方法**和**非参数方法**：

   * **参数方法**

     参数方法指有一定的形式或形状的模型，如假设$f(\cdot)$是线性的，则具有如下形式：
     $$
     f(X)=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p
     $$
     在模型选完后则需要使用训练数据去**拟合（fit）**或**训练（train）**模型，即估计参数$\beta_0,\beta_1,...,\beta_p$。参数方法**最大的优势**就是可以将$f(\cdot)$假设为具体的参数形式可简化估计。然而**缺陷**则是选定的模型并非与真正 的$f(\cdot)$在形式上是一致的。**非参数方法适合推断的问题。**

   * **非参数方法**

     非参数方法不需要对函数$f$的形式事先做明确的假设。**优势**是不限定函数$f(\cdot)$的具体形式，可能在更大的范围选择更适宜$f(\cdot)$，然而有**最致命的缺陷**即无法将估计$f(\cdot)$的问题简化成对参数的估计，需要大量的数据（远远超出参数方法所需要的）。

4. **监督学习**和**非监督学习**的区别在于**前者有响应变量（标签）**，形如$\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$而**后者无响应变量（标签）**，形如$\{x_1,x_2,...,x_n\}$。
5. 根据变量的**定量（连续）**和**定性（离散）**类型，可将任务分为**回归**和**分类**问题，前者如对GDP、PM2.5的预测，后者如对动物、生病与否的识别。

## 2.2题目答案

1. (a) 当**样本量n非常大**，**预测变量数p很小**时，这样容易欠拟合，所以一个**光滑度更高的学习模型更好**。

   (b) 当**样本量n非常小**，**预测变量数p很大**时，这样容易过拟合，所以一个**光滑度更小的学习模型更好**。

   (c) 当预测变量与响应变量之间的关系是**非线性**时，说明光滑度小的模型会容易欠拟合，所以**光滑度高的模型更适合**。

   (d) 当**误差项的方差$\sigma^2=Var(\epsilon)$极大**时，因为方差是指用一个不同的训练数据集估计f时，估计函数的改变量。一般来说，**光滑度越高的统计模型有更高的方差**，所以这里适合**光滑度小的模型**。

2. (a) 收集了美国500强公司的数据。每个公司都记录了利润、员工人数、产业类型和CEO的工资。（回归，推断）

   (b) 考虑研发一个新产品，希望知道它会成功还是失败，收集了先前研发的20个相近产品的数据，并记录它们成功或失败的状态，以及其他若干变量。（分类，预测）

   (c) 兴趣在于预测美元的百分比变化率随全球股市周变动的变化规律，为此收集了2012年所有的周数据。（回归，预测）

3. **偏差：**度量了学习算法的期望预测与真实结果偏离程度，即刻画了学习算法本身的拟合能力。

   **方差：**度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响，或者说学习算法的稳定性。

   **训练误差：**模型在训练集上的误差。

   **测试误差：**测试集上的误差。

   **贝叶斯（或不可约）误差：**贝叶斯误差也叫最优误差，通俗来讲，它指的就是现有技术下人和机器能做到最好的情况下，出现的误差。比如图像识别和语音识别这类处理自然数据的任务，人类水平和贝叶斯水平相差不远，通常用人类水平来近似成贝叶斯水平，也就是说人的误差可以近似地看成贝叶斯误差。

   **<u>偏差 = 贝叶斯误差 + 可避免偏差</u>**

   <img src="images\ch2\3(a).jpg" style="zoom:50%;" />

4. 略

5. 一个光滑度高的回归模型或者分类模型，能够更好的拟合非线性模型，偏差更小。但是模型越光滑，所需要计算的参数就越多，而且容易过拟合，方差更大。当我们更想预测，而不是推断的时候，我们优先考虑光滑度高的模型。光滑度低的模型相反。

6. (a) 参数方法是一种基于模型估计的两阶段方法。优点：它把估计$f(\cdot)$的问题简化到估计一组参数，对f假设一个具体的参数形式将简化对$f(\cdot)$的估计，因为估计参数是更为容易的，不需要拟合任意一个函数$f(\cdot)$。缺点：选定的模型并非与实际的f形式上一致，而且还有过拟合的可能情况。

   (b) 非参数方法不需要对函数f的形式实现做明确说明的假设。相反，这类方法追求的接近数据点的估计，估计函数在去粗和光滑处理后尽量可能与更多的数据点接近。优点：不限定函数$f(\cdot)$的具体形式，可以更大的范围选择更适宜的$f(\cdot)$形状的估计。缺点：无法将估计$f(\cdot)$的问题简单到对少数参数进行估计的问题，所以往往需要大量的观察点。

# 第三章 线性回归

## 3.1 相关概念

### 3.1.1 简单线性回归

1. **简单线性回归（Simple linear regression）**假定$X$和$Y$之间存在线性关系，其形式为：
   $$
   Y\approx\beta_0+\beta_1X
   $$
   表示**$Y$对$X$的回归**，其中$\beta_0$和$\beta_1$分别表示为模型的截距和斜率，被称为模型的**系数（coefficient）**或**参数（parameter）**。在给定数据时，也可表示为：
   $$
   \hat{y}=\hat{\beta}_0+\hat{\beta}_1x
   $$
   其中，$\hat{y}$表示$X=x$的基础上对$Y$的预测。

2. 评价模型拟合效果可通过测量**接近程度（closeness）**，常用观测的相应值和预测的相应值之间的差距作为参考，定义**残差平方和（Residual sum of square，RSS）**为：
   $$
   \begin{align}
   RSS=&e_1^2+e_2^2+...+e_n^2\\
   =&(y_1-\hat{\beta_0}-\hat{\beta}_1x_1)^2+(y_2-\hat{\beta_0}-\hat{\beta}_1x_2)^2+...+(y_n-\hat{\beta_0}-\hat{\beta}_1x_n)^2\\
   =&\sum_{i=1}^n(y_i-\hat{y}_i)^2
   \end{align}
   $$

3. **最小二乘估计（Least squares coefficient estimate）**期望将模型的RSS达到最小，如图所示，其中每条线段代表一个残差。

   <img src="images\ch3\3-1.png" style="zoom:50%;" />

   可通过微积分运算，使简单线性回归的RSS达到最小的参数估计为：
   $$
   \hat{\beta}_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\\
   \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}
   $$
   其中，$\bar{y}\equiv\frac{1}{n}\sum_{i=1}^ny_i$和$\bar{x}\equiv\frac{1}{n}\sum_{i=1}^nx_i$是样本均值。

   >**最小二乘估计的推导**
   >$$
   >\begin{align}
   >&RSS=\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta}_1x_i)^2 \\
   >\\
   >\overset{对参数求偏导}{\Rightarrow}
   >&\left\{\begin{aligned}
   >\frac{\partial{RSS}}{\partial{\hat{\beta}_0}}=&2\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta}_1x_i)\\ 
   >\frac{\partial{RSS}}{\partial{\hat{\beta}_1}}=&2\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta}_1x_i)(-x_i) \\
   >\end{aligned}\right.\\
   >\\
   >\overset{令其为0}{\Rightarrow}
   >&\left\{\begin{aligned}
   >&\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta}_1x_i)&=0\\ 
   >&\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta}_1x_i)(x_i)&=0 \\
   >\end{aligned}\right.\\
   >\\
   >\overset{化简得}{\Rightarrow}
   >&\left\{\begin{aligned}
   >&n\bar{y}-n\hat{\beta}_0-n\hat{\beta}_1\bar{x}&=0 \\ 
   >&\sum_{i=1}^nx_iy_i-n\hat{\beta}_1\bar{x}-\hat{\beta}_1\sum_{i=1}^nx_i^2&=0 \\
   >\end{aligned}\right.\\
   >\\
   >\overset{化简得}{\Rightarrow}
   >&\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}\\
   >&\hat{\beta}_1=\frac{\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}^nx_i^2-n\bar{x}^2}\\
   >&\quad=\frac{\sum_{i=1}^nx_iy_i-\sum_{i=1}^n\bar{x}\bar{y}}{\sum_{i=1}^nx_i^2-\sum_{i=1}^n\bar{x}^2}\\
   >&\quad=\frac{\sum_{i=1}^n(x_iy_i-yi\bar{x}-x_i\bar{y}+\bar{x}\bar{y})}{\sum_{i=1}^n(x_i^2-2x_i\bar{x}+\bar{x}^2)}\\
   >&\quad=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
   >\end{align}
   >$$

4. 线性回归是**无偏估计**，同时也遵从估计的**相合性**（遵循格里纹科定理）原则，即如果在特定数据集的基础上估计$\beta_0$和$\beta_1$，则估计值不会恰等于$\beta_0$和$\beta_1$，但是，如果对从大量数据集上得到的估计值求平均，他们的均值恰为真值。

5. 一般的估计问题中，可以使用**标准误差（Standard error， 写作$SE(\hat{\mu})$）**评价估计的准确性，表示估计$\hat{\mu}$偏离$\mu$的实际值的平均量，形式为：
   $$
   Var（\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}
   $$
   其中，$\sigma$是变量$Y$的每个现实值$y_i$的标准差。同理，也可以探究$\hat{\beta}_0$和$\hat{\beta}_1$与真实值$\beta_0$和$\beta_1$的接近程度，形如：
   $$
   SE(\hat{\beta}_0)^2=\sigma^2[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}]\\
   SE(\hat{\beta}_1)^2=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}
   $$

   > **系数标准误差的推导：**
   >
   > 暂略，后续补

6. 标准误差可用于计算**置信区间（Confidence interval）**，对于线性回归模型，$\beta_1$和$\beta_0$的95%置信区间约为$\hat{\beta}_1\pm2\cdot SE(\hat{\beta}_1)$和$\hat{\beta}_0\pm2\cdot SE(\hat{\beta}_0)$.

7. 标准误差可用对系数进行**假设检验**，其中最常用的检验包括零假设（$X$和$Y$之间没有关系）和备择假设（$X$和$Y$之间有一定的关系），使用$t$统计量测量$\hat{\beta}_1$偏离0的标准偏差，其形式为：
   $$
   t=\frac{\hat{\beta}_1-0}{SE(\hat{\beta}_1)}
   $$
   对于零假设，即假设$\beta_1=0$，计算任意观测值大于等于$|t|$的概率即可，该概率为$p$**值**，可以解释为：一个很小的p值表示，在预测变量和相应变量之间的真实关系未知的情况下，不太可能完全由于偶然而观察到预测变量和相应变量之间的强相关。因此，**如果$p$值很小，可以推断预测变量和相应变量之间存在关联，即可拒绝零假设**，典型的拒绝零假设的临界$p$值是5%或1%.

8. 评价模型的准确性有两个指标，一是**残差标准误（Residual standard error，$RSE$）**，是对模型中$\epsilon$的标准偏差的估计，形式为：
   $$
   RSE=\sqrt{\frac{1}{n-1}RSS}=\sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y})^2}
   $$
   RSE被认为是对模型**失拟（lack of fit）**的度量，$\hat{y}_i$与$y_i$相差很大，那么RSE可能是相当大的，这表明该模型未能很好地拟合数据。

9. 另一是**$R^2$统计量**，相比较于RSE对数据失拟的绝对测度方法，$R^2$统计量采取比例（被解释方差的比例）形式，其形式为：
   $$
   R^2=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}
   $$
   其中，$TSS$是总平方和，测量了相应变量$Y$的总方差，可认为是在执行回归分析之前相应变量中的固有变异性，$RSS$测量的是进行回归后仍无法解释的变异性，因此$TSS-RSS$测量的是相应变量进行回归之后被解释（或被消除）的变异性，则**$R^2$测量的是$Y$变异中能被$X$解释的部分所占比例**。

   >**注：**在简单线性回归中，$R^2$统计量等价于$X$和$Y$的相关系数，即$R^2=r^2$，证明如下：
   >$$
   >\begin{align}
   >Cor(X, Y)&=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}\\
   >\\
   >R^2&=\frac{\sum_{i=1}^n(y_i-\bar{y})^2-\sum_{i=1}^n(y_i-\hat{y})^2}{\sum_{i=1}^n(y_i-\bar{y})^2}\\
   >\\
   >令A&=\sum_{i=1}^n(y_i-\bar{y})^2-\sum_{i=1}^n(y_i-\hat{y})^2\\
   >&=\sum_{i=1}^n[(y_i-\bar{y})^2-(y_i-\hat{y})^2]\\
   >&=\sum_{i=1}^n(y_i-\bar{y}-y_i+\hat{y}_i)(y_i-\bar{y}+y_i-\hat{y}_i)\\
   >&=\sum_{i=1}^n(\hat{y}_i-\bar{y})(2y_i-\bar{y}-\hat{y}_i)\\
   >&=\sum_{i=1}^n(\hat{\beta}_0+\hat{\beta}_1x_i-\bar{y})(2y_i-\bar{y}-\hat{\beta}_0-\hat{\beta}_1x_i)\\
   >&=\sum_{i=1}^n\hat{\beta}_1(x_i-\bar{x})[2y_i-2\bar{y}-\hat{\beta}_1(x_i-\bar{x})]\\
   >&=\hat{\beta}_1[2\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})-\hat{\beta}_1\sum_{i=1}^n(x_i-\bar{x})^2]\\
   >&=\hat{\beta}_1\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})\\
   >&=\frac{[\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})]^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\\
   >\\
   >\therefore R^2&=\frac{A}{\sum_{i=1}^n(y_i-\bar{y})^2}=\frac{[\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})]^2}{\sum_{i=1}^n(x_i-\bar{x})^2\sum_{i=1}^n(y_i-\bar{y})^2}=Cor^2
   >\end{align}
   >$$
   >

### 3.1.2 多元线性回归

1. 多元线性回归涉及了$p$个不同的预测变量，该模型形式为：
   $$
   Y=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p+\epsilon
   $$
   其中，$X_j$代表第$j$个预测变量，$\beta_j$代表第$j$预测变量和相应变量之间的关联。在给定数据时，其参数估计形式为：
   $$
   \hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\hat{\beta}_2x_2+...+\hat{\beta}_px_p
   $$

2. 多元线性回归的最小二乘估计原理同简单线性回归一样，期望将模型的RSS达到最小，如图所示
   

<img src="images\ch3\3-4.png" style="zoom:50%;" />

   但是需要用矩阵代数形式表示：
$$
\pmb{\hat{\beta}}=(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\pmb{y}
$$
   其中，$\pmb{\hat{\beta}}$为参数向量，$\pmb{X}$为预测变量矩阵，$\pmb{y}$为响应变量向量。

   >**最小二乘估计的推导**
   >$$
   >\begin{align}
   >\pmb{y}&=\pmb{X}\pmb{\beta}\\
   >RSS&=(\pmb{y}-\pmb{X}\hat{\pmb{\beta}})^T(\pmb{y}-\pmb{X}\hat{\pmb{\beta}})\\
   >&=\pmb{y}^T\pmb{y}-\pmb{y}^T\pmb{X}\pmb{\hat{\beta}}-\pmb{\hat{\beta}}^T\pmb{W}^T\pmb{y}+\pmb{\hat{\beta}}^T\pmb{X}^T\pmb{X}\pmb{\hat{\beta}}\\
   >\\
   >由\frac{\part{\pmb{a}^T\pmb{x}}}{\part{\pmb{x}}}&=\frac{\part{\pmb{x}^T\pmb{a}}}{\part{\pmb{x}}}=\pmb{a},\quad \frac{\part{\pmb{x}^T\pmb{A}\pmb{x}}}{\part{\pmb{x}}}=(\pmb{A}+\pmb{A}^T)\pmb{x}\quad得\\
   >\\
   >\frac{\part{RSS}}{\part{\pmb{\hat{\beta}}}}&=0-\pmb{X}^T\pmb{y}-\pmb{X}^T\pmb{y}+(\pmb{X}^T\pmb{X}+\pmb{X}^T\pmb{X})\pmb{\hat{\beta}}\\
   >&=2\pmb{X}^T(\pmb{X}\pmb{\hat{\beta}}-\pmb{y})\\
   >\\
   >当满秩时，令\frac{\part{RSS}}{\part{\pmb{\hat{\beta}}}}&=0\\
   >\therefore \pmb{X}^T\pmb{X}\pmb{\hat{\beta}}-\pmb{X}^T\pmb{y}&=0\\
   >\pmb{\hat{\beta}}&=(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\pmb{y}
   >\end{align}
   >$$

3. 虽然响应变量和预测变量在简单线性回归中具有较高的$R^2$，但**若增加其他预测变量后，原始的预测变量可能将不具有统计意义（$p$值低）**，这是因为简单回归模型忽视了预测变量之间的相互关系，可能存在着内部联系。
4. 多元线性回归将关注一下几个重要问题：
   * 预测变量$X_1,X_2,...,X_p$中是否至少有一个可以用来预测响应变量？
   * 所有预测变量都有助于解释$Y$吗？或仅仅是预测变量的一个子集对预测有用？
   * 模型对数据的拟合程度如何？
   * 给定一组预测变量的值，响应值应预测为多少？所作预测的准确程度如何？

5. 在简单线性回归中，可以使用$p$值衡量模型的有效性，但$p$值是针对每一个预测变量的，且**在实际中，即使任何预测变量与响应变量都不相关，但仍有很小的几率使得部分$p$值小于0.05，因此单独使用$t$统计量和$p$值将很有可能错误地得出相关性的结论**。因此需要计算模型整体的评价指标，$F$统计量：
   $$
   F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}
   $$
   因为$E\{RSS/(n-p-1)\}=\sigma^2$，若零假设为真，$E\{TSS-RSS)/p\}=\sigma^2$，所以**当响应变量和预测变量无关时，$F$统计量应接近1**。一个较大的$F$统计量表示，至少有一个预测变量与响应变量相关，**若$n$很大，即使$F$统计量只是略大于1，可能也仍然提供了拒绝零假设的证据，相反，若$n$很小，则需要较大的$F$统计量才能拒绝零假设**。

6. 对于本章模型，若$p<n$，则可以使用相应的评价指标，如$F$统计量等，相反，不可以使用上述的指标，因为无法使用最小二乘法估计模型参数。

7. 在多元线性回归模型中，**最常见的情况是响应变量仅与预测变量的一个子集相关。**因此，需要对预测变量进行筛选，当预测变量很少时，可一个一个迭代筛选，但是数量较多时，则需要以下方法：

   * **向前选择**：从**零模型**（只包含截距）开始，对所有预测变量与响应变量建立简单线性回归模型，并将RSS最小的预测变量纳入零模型中，直到满足某种规则时停止。
   * **向后选择**：从包含所有变量的模型开始，依次删除$p$最低的变量，循环操作，知道满足某种规则时停止。
   * **混合选择**：先做向前选择，在做向后选择。

   > **注：**当$p>n$时，不能使用向后选择，而向前选择在各种情况下都适用。向前选择基于贪婪的模式，可将对模型没有“贡献”的变量纳入其中，可使用混合选择方法修正该问题。

8. 在简单回归中，$R^2$是响应变量和预测变量的相关系数的平方，在多元线性回归中，$R^2=Cor(Y,\hat{Y})^2$，即是响应值和线性模型拟合值的相关系数的平方。（其实本质上是一样的，在简单线性回归中，$\hat{Y}$只是$X$的线性变换，并不影响相关系数。）
9. 在多元线性回归中，预测变量间可能存在**协同效应（synergy）**或**交互作用（interaction）**，即组合使用这些预测变量比单独使用预测变量效果更好。

### 3.1.3注意事项

1. **定性预测变量**。大多数预测变量都是定量的（或者说是**连续型数据**），但有时预测变量会是定性的（或者说是**离散型数据**），如性别（男女）、种族（黄人、白人、黑人）等。以最简单的**二值预测变量**为例，可以将其创建**哑变量**（dummy variable），如基于性别变量创建新变量：
   $$
   x_i=\left\{\begin{aligned}
   1\quad 女性\\
   0\quad 男性
   \end{aligned}\right.\\
   $$
   回归模型可以表示为：
   $$
   y_i=\beta_0+\beta_1x_i+\epsilon_i=\left\{\begin{aligned}
   \beta_0+\beta_1+\epsilon_i \quad 女性\\
   \beta_0+\epsilon_i \quad 男性
   \end{aligned}\right.\\
   $$
   其中，$\beta_0$可以解释为男性的平均值，$\beta_0+\beta_1$为女性的平均值，$\beta_1$是男性和女性之间的差异值。

2. 标准的线性回归模型有两个重要的假设，即预测变量和相应变量是**可加**和**线性**的。前者假设预测变量之间是互相**独立**分布的，后者假设无论预测变量取何值，该预测变量引起相应变量的变化是**恒定**的。

   但是，在现实中并不满足这样的假设，比如预测变量之间**高度相**关，存在协同效应或交互作用，亦或预测变量和相应变量之间的真实关系并是**非线性**的，针对前者需要对变量进一步筛选或降维，后者则需要将模型假设修正为非线性。

3. 下面将介绍线性模型遇到的常见几个问题，分别是**数据本身存在非线性**、**误差项自相关**、**误差项方差非恒定**、**离群点**、**高杠杆点**和**共线性**。

   * **数据本身存在非线性**

     * 实际情况中，很少数据是满足线性的，可以根据**残差图**（Residual plot）识别非线性，如下图所示，左图的残差趋势为U形，表明真实的关系应该是非线性的，当将模型修正为非线性时，残差呈现随机分布，表明该修正提升了模型对数据的拟合度。

       <img src="images\ch3\3-9.png" style="zoom:50%;" />

     * **措施：**对预测变量使用费线性变换，如$logX, \sqrt{X}, X^2$，或者使用更先进的非线性方法。

   * **误差项自相关**
     * 如果误差项存在相关性，那么标准误的估计往往会低估了真实标准误。以时间序列或者是地理数据为例，这两类数据最为明显，相邻的观测呈现误差正相关的关系。
     * **措施：**差分法等。

   * **误差项方差非恒定**

     * 线性回归模型的另一个重要假设是误差项的方差是恒定的，$Var(\epsilon_i)=\sigma^2$.但通常情况下，误差项的方差并不恒定，可能随着相应值的增加而增加。如图所示，左图残差图呈漏斗形，表明误差方差非恒定。

       <img src="images\ch3\3-11.png" style="zoom:50%;" />

     * **措施：**使用凹函数对相应值做变换，如$logY$和$\sqrt{Y}$，结果如上图右边所示。

   * **离群点**

     * 离群点是指$y_i$远离模型预测值的点，可通过残差图识别离群点，但是难以使用定量化的方法描述离群点，为解决该问题，引入了**学生化残差**，其由残差$e_i$除以它的估计标准误得到。**学生化残差绝对值大于3的观测点可能是离群点**。
     * **措施：**直接剔除此观测点。**但是，一个离群点可能不是由失误导致的，而是暗示模型存在缺陷，比如缺少预测变量。**

   * **高杠杆点**

     * 高杠杆表示观测点$x_i$是异常的，如下图左边，观测点41具有高杠杆值，因为它的预测变量值比其他观测点都要大。**高杠杆点的观测往往对回归直线的估计有很大的影响（比离群点还大）**。

       <img src="images\ch3\3-13.png" style="zoom:50%;" />

       虽然在各预测变量的取值都在正常范围内，但从整体预测变量集的角度来看，它却是不寻常的，如上图中部所示。可以计算**杠杆统计量**：
       $$
       h_i=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum^n_{i'=1(x_{i'}-\bar{x})^2}}
       $$
       一个大的杠杆统计量对应一个高杠杆点，杠杆统计量$h_i$的取值总是在$\frac{1}{n}$和1之间，且所有观测的平均杠杆值总是等于$(p+1)/n$。

     * **措施：**剔除。

   * **共线性**

     * 共线性指两个或更多的预测变量高度相关。检测共线性的一个简单方法是看预测变量的相关系数矩阵，但当有多重共线性时（三个或更多变量间存在共线性），更优的检测方法是计算方差膨胀因子（Variance inflation factor，VIF）（不做介绍）。
     * **措施：**1.剔除；2.降维。

## 3.2 题目答案

1. 电视和广播的低p值表明对于电视和广播的零假设都是错误的。报纸的高p值表明对于报纸的零假设是正确的，即报纸该预测变量不具有统计意义。

2. 差距既是回归和分类任务之间的差距，前者适合连续变量的预测，后者适合离散变量的预测。

3. (a)和(b)略

   (c)错误，不能直接通过回归系数评判交互项的有效性，需要通过该交互项的p值。

4. 一组数据包括单个预测变量和定量响应变量（观测数=100），分别使用线性回归模型和三次项回归模型进行拟合

   (a)假设X和Y满足线性关系，三次项回归模型的训练RSS小于线性回归模型的训练RSS（因为高次模型的误差项更高）；

   (b)条件同(a)，三次项回归模型的测试RSS大于线性回归模型的测试RSS（因为真实的关系为线性）；

   (c)假设X和Y满足非线性关系且具体关系未知，三次项回归模型的训练RSS小于线性回归模型的训练RSS（因为高次模型的误差项更高）；

   (d)条件同(c)，则不能判断谁的测试RSS低，因为并不知道真实的关系离线性近还是离三次近。

5. 设$\hat{y}_i=x_i\hat{\beta}$，其中$\hat{\beta}=(\sum^n_{i=1}x_iy_i)/(\sum^n_{i'=1}x^2_{i'})$，证明：$\hat{y}_i=\sum^n_{i'=1}a_{i'}y_{i'}$

   证：
   $$
   \begin{align}
   \hat{y}_i=&x_i\hat{\beta}\\
   =&x_i\frac{\sum^n_{k=1}x_ky_k}{\sum^n_{z'=1}x^2_{z'}}\\
   =&\sum^n_{k=1}\frac{x_ix_k}{\sum^n_{z'=1}x^2_{z'}}y_k\\
   =&\sum^n_{i'=1}a_{i'}y_{i'}
   \end{align}
   $$

6. 在简单线性回归中，最小二乘线通过点$(\bar{x},\bar{y})$.

7. 证明简单线性回归中的$R^2$统计量等于X和Y之间的相关系数的平方。

# 第四章 分类

## 4.1 相关概念

### 4.1.1 分类问题概述

1. 分类问题是针对定性变量的，大部分基于不同类别的概率，将分类问题作为**概率估计**的一个结果。
2. 当类别数较多（大于2类），则线性回归不具有意义，因为类别数无法被定量表达，例如：1代表红色，2代表绿色，3代表蓝色，则使这些类型具有可度量性，与实际不符，但对于2分类（0-1分类）来说，线性回归具有一定的意义，但预测结果很容易超过0-1范围。

### 4.1.2 逻辑斯蒂回归

1. **逻辑斯蒂回归（Logistic regression）**可以看成是线性回归的推广（广义线性回归），针对2分类问题，是神经网络中重要部分（激活函数，Sigmoid）。其形式为：
   $$
   p(X)=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}} \tag{4.1}
   $$
   <img src="images\ch4\4-2.png" style="zoom:40%;" />

   当概率超过阈值时，则为正类，小于阈值时为负类。

2. 4.1式可整理为$\frac{p(X)}{1-p(X)}=e^{\beta_0+\beta_1X}$，其中$\frac{p(X)}{1-p(X)}$称为**几率（odd）**，取值范围为0到$\infty$，对其两边取对数可得$log(\frac{p(X)}{1-p(X)})=\beta_0+\beta_1X$，因此，逻辑斯蒂回归可以视为分对数变换下关于X的线性回归模型，逻辑斯蒂模型也较为**对数几率回归**。（参考《机器学习》—周志华）

3. 逻辑斯蒂回归**对Y属于某一类的概率建模**，而不直接对响应变量Y建模。

4. 估计回归系数可使用极大似然估计，似然函数为：
   $$
   L(\beta_0,\beta_1)=\prod _{i:y_i=1}p(x_i)\prod _{i':y_{i'}=0}(1-p(x_{i'}))
   $$

   > 推导：
   > $$
   > \begin{align}
   > 似然函数：L(w)&=\prod_{i=1}^N[p(x_i)]^{y_i}[1-p(x_i)]^{1-y_i}\\
   > 对数似然函数：\mathbb{L}&=\sum_{i=1}^{N}[y_ilog\ p(x_i)+(1-y_i)log(1-p(x_i))]\\
   > &=\sum_{i=1}^{N}[y_ilog\ \frac{p(x_i)}{1-p(x_i)}+log(1-p(x_i))]\\
   > &=\sum_{i=1}^{N}[y_i(w\cdot x_i)-log(1+e^{(w\cdot x_i)})]\\
   > \end{align}
   > $$
   > 使用梯度下降算法或牛顿法求解对数似然函数的极大值

   > 注：极大似然估计可参考另一个笔记《概率论与数理统计笔记》https://github.com/QianXzhen/Statistics-note

### 4.1.3 线性判别分析和二次判别分析（LDA和QDA）

> 注：本书中是以统计（贝叶斯决策理论）角度来阐释的，从线性空间角度可以参考《机器学习》—周志华

1. **贝叶斯定理（Bayes theorem）**可以表述为
   $$
   p_k(X)=\frac{\pi_kf_k(x)}{\sum^K_{l=1}\pi_lf_l(x)}
   $$
   其中，$p_k(X)=P(Y=k|X=x)$表示$X=x$的观测属于第$k$类的后验概率，$f_k(X)=P(X=x|Y=k)$表示第$k$类观测的$X$的密度函数，$\pi_k$为一个随机选择的观测来自$k$类的先验概率。**贝叶斯分类起将观测分到$p_k(X)$最大的一类中。**

2. **单预测变量线性判别分析**，假设$f_k(x)$是**正态的或高斯的**，一维情况其密度函数为
   $$
   f_k(x)=\frac{1}{\sqrt{2\pi}\sigma_k}\exp{(-\frac{1}{2\sigma^2_k}(x-\mu_k)^2)}
   $$
   其中，$\mu_k$和$\sigma^2_k$是第$k$类的平均值和方差，且假设所有**方差是相等**的，则
   $$
   p_k(x)=\frac{\pi_k\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2\sigma^2}(x-\mu_l)^2)}
   $$
   化简得到等价式
   $$
   \delta_k(x)=x\cdot \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k)
   $$
   > 证明：
   > $$
   > \begin{align}
   > &\left\{\begin{aligned}
   > p_k(x) &= \frac {\pi_k
   >                 \frac {1} {\sqrt{2 \pi} \sigma}
   >                 \exp(- \frac {1} {2 \sigma^2} (x - \mu_k)^2)
   >                }
   >                {\sum {
   >                 \pi_l
   >                 \frac {1} {\sqrt{2 \pi} \sigma}
   >                 \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
   >                }}\\
   > \delta_k(x) &= x \frac {\mu_k} {\sigma^2} - \frac {\mu_k^2} {2 \sigma^2}
   >               + \log(\pi_k)\\
   > \end{aligned}\right.\\
   > 
   > &令c = \frac {
   >                 \frac {1} {\sqrt{2 \pi} \sigma}
   >                 \exp(- \frac {1} {2 \sigma^2} x^2)
   >                }
   >                {\sum {
   >                 \pi_l
   >                 \frac {1} {\sqrt{2 \pi} \sigma}
   >                 \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
   >                }}\\
   > 
   > &\therefore p_k(x) = C \pi_k \exp(- \frac {1} {2 \sigma^2} (\mu_k^2 - 2x \mu_k))\\
   > &\therefore log(p_k(x)) = log(C) + log(\pi_k) + (- \frac {1} {2 \sigma^2} (\mu_k^2 - 2x \mu_k))\\
   > &\therefore log(p_k(x)) =  (\frac {2x \mu_k} {2 \sigma^2} -\frac {\mu_k^2} {2 \sigma^2}) + log(\pi_k) + log(C)\\
   > &\because C不随着k的变化而改变，其值是一个定值\\
   > &\therefore 令\delta_k(x)=(\frac {2x \mu_k} {2 \sigma^2} -\frac {\mu_k^2} {2 \sigma^2}) + log(\pi_k)，最大化p_k(x)等价最大化\delta_k(x)
   > \end{align}
   > $$
   
   对于贝叶斯分类只要$\delta_k(x)$达到最大即可。与之相同，因为并不知道原始的参数，需要通过样本估计$\mu_k$、$\sigma$和$\pi_k$，其中
$$
\begin{align}
   \hat{\mu}_k&=\frac{1}{n_k}\sum_{i:y_i=k}x_i\\
   \hat{\sigma}^2&=\frac{1}{n-K}\sum_{k=1}^K \sum_{i:y_i=k}(x_i-\hat{\mu}_k)^2\\
   \hat{\pi}_k&=\frac{n_k}{n}
   \end{align}
$$
​		将这些估计值代入$\delta_k(x)$中，得到$\hat{\delta}_k(x)$，找到令其值最大的类别$k$作为观测的预测类别。

3. **多预测变量线性判别分析**，其原理和但预测变量相同，但是基于多元预测变量，所以均值和方差都变成了**均值向量**、**协方差矩阵**，多元高斯分布密度函数形式为
   $$
   f(x)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp{(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}
   $$
   $\delta_k(x)$形式为
   $$
   \delta_k(x)=x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k+\log\pi_k
   $$

4. **二次判别分析**与LDA不同之处在于不假设各样本的协方差矩阵（方差）相等，每类观测都有自己的协方差矩阵，形式为
   $$
   \delta_k(x)=-\frac{1}{2}x^T\Sigma_k^{-1}x+x^T\Sigma_k^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k+\log\pi_k
   $$
   QDA和LDA的关系如同非线性回归和线性回归的关系，归属于方差和偏差权衡的问题。

### 4.1.4 ROC曲线

> 参考知乎回答：https://www.zhihu.com/question/39840928/answer/241440370（作者：无涯）

1. 混淆矩阵中有着Positive、Negative、True、False的概念，其意义如下：

   - 称预测类别为1的为Positive（阳性），预测类别为0的为Negative（阴性）。
   - 预测正确的为True（真），预测错误的为False（伪）。

   <img src="images\ch4\confuse.png" style="zoom:100%;" />

**然后**，由此引出True Positive Rate（真阳率）、False Positive（伪阳率）两个概念：

* $TP=\frac{TP}{TP+FN}$，指所有真实类别为1的样本中，预测类别为1的比例；
* $FP=\frac{FP}{FP+TN}$，指所有真实类别为0的样本中，预测类别为1的比例。

2. ROC曲线的横轴是FPRate，纵轴是TPRate，当二者相等时，即y=x，如下图

   <img src="images\ch4\ROC.png" style="zoom:100%;" />

   一个理想的ROC曲线会紧贴左上角，即期望真阳率为1，假阳率为0.

3. AUC（area under the ROC）是ROC曲线下的面积，其最小值为0.5，即上图的面积，一个理想的ROC曲线的AUC为1，AUC的优势是**AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价**。

   例子：

   例如在反欺诈场景，设欺诈类样本为正例，正例占比很少（假设0.1%），如果使用准确率评估，把所有的样本预测为负例，便可以获得**99.9%的准确率**。

   但是如果使用AUC，把所有样本预测为负例，TPRate和FPRate同时为0（没有Positive），与(0,0) (1,1)连接，得出**AUC仅为0.5**，成功规避了样本不均匀带来的问题。

## 4.2 题目答案

1. 证明逻辑斯蒂函数表达式和分对数表达式等价
   $$
   \begin{align}
   &\left\{\begin{aligned}
   P(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\\
   \frac {p(X)} {1 - p(X)} =e^{\beta_0 + \beta_1 X}\\
   \end{aligned}\right.\\
   
   下证：\\
   \frac {p(X)} {1 - p(X)}&= \frac {\frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}
           {1 - \frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}
   \\
   \\
   &= \frac {\frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}
           {
             \frac {1 + e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}
             - \frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}
           }
   \\
   \\
   &= \frac {\frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}
           {\frac {1} {1 + e^{\beta_0 + \beta_1 X}}}\\
   \\
   &=e^{\beta_0 + \beta_1 X}\\
   \end{align}
   $$
   
2. 贝叶斯将观测分入最大概率类别中$p_k(x)$和$\delta_k(x)$等价，已证。

3. 设一类观测服从均值向量不同、协方差矩阵不等的正态分布，考虑只有一元变量，有K类观测，证明该种情况下，贝叶斯分类起不是线性的，是二次的。
   $$
   p_k(x) = \frac {\pi_k \frac {1} {\sqrt{2 \pi} \sigma_k} \exp(- \frac {1} {2 \sigma_k^2} (x - \mu_k)^2) } {\sum { \pi_l \frac {1} {\sqrt{2 \pi} \sigma_l} \exp(- \frac {1} {2 \sigma_l^2} (x - \mu_l)^2) }}\\
   令C' = \frac { \frac {1} {\sqrt{2 \pi}}} {\sum { \pi_l \frac {1} {\sqrt{2 \pi} \sigma_k} \exp(- \frac {1} {2 \sigma_k^2} (x - \mu_l)^2) }}\\
   \therefore p_k(x) = C' \frac{\pi_k}{\sigma_k} \exp(- \frac {1} {2 \sigma_k^2} (x - \mu_k)^2)\\
   \therefore log(p_k(x)) = log(C') + log(\pi_k) - log(\sigma_k) + (- \frac {1} {2 \sigma_k^2} (x - \mu_k)^2)\\
   $$
   所以$log(p_k(x))$是关于$x$的二次函数。

4. 当变量维数$p$很大时，只用测试观测附近的观测去做预测的局部方法效果都不理想，这种现象称为**维数灾难**（curse of dimensionality），即当$p$很大时，非参数模型效果很差。
5. LDA v.s. QDA
   * 如果**贝叶斯决策边界是线性**的，则训练集上QDA比LDA的效果好，测试集上LDA比QDA的效果好；
   * 如果**贝叶斯决策边界是非线性**的，则训练集和测试集上QDA比LDA的效果好；
   * 在一般情况下，随着样本量$n$增大，相比于LDA的测试预测率，QDA的预测率将变得更好，因为较大的样本量可以抵消方差，避免过拟合；
   * 如果贝叶斯边界是线性的，应该使用LDA，不能因为QDA的光滑度高而选用。

6-9. 略

# 第五章 重抽样方法

&emsp;&emsp;重抽样（resampling）通过反复从训练集中抽取样本，然后根据每一个样本重新拟合一个新模型，以此获得该模型的评价信息。重采样方法选择模型时，不是为了完美地估计出真实测试错误率，而是选择较优的超参数，使得模型能达到或逼近最低测试错误率。

## 5.1 相关概念

### 5.1.1 交叉验证法（Cross Validation）

1. **验证集法（validation set approach）**，将获得的观测数据分为两个大小相当的子集：**训练集（training set）**和**验证集（validation set）**，或者说**保留集（hold-out set）**，其原理下图所示。

   <img src="images\ch5\5-1.png" style="zoom:50%;" />

   验证集方法**优点****原理简单，易于执行，但有两个潜在的**缺陷**：1.模型的测试错误率波动大；2.验证集错误率可能高估真实测试错误率。

2. **留一交叉验证法（leave-one-out cross-validation，LOOCV）**，该方法将当一个单独的观测作为验证集，剩下的n-1个观测组成训练集，重复该步骤n次，最后测试均方误差的LOOCV估计是这n个测试误差估计的均值：$CV_{(n)}=\frac{1}{n}\sum_{i=1}^nMSE_i$，其原理如下图所示。

   <img src="images\ch5\5-2.png" style="zoom:50%;" />

   LOOCV方法有以下几个**优点**：1.估计的偏差小；2.由于LOOCV方法在训练集和验证集的分割上不存在随机性，因此多次运用LOOCV方法总会得到相同的结果。**缺点**：1.计算量很大，模型需要被拟合n次；2产生的测试误差估计的方差比k折CV法的大。

   > 注：用最小二乘法拟合线性或者多项式回归模型时，LOOCV方法所花费的时间将缩减至与拟合一个模型相同，公式如下：
   > $$
   > CV_{(n)}=\frac{1}{n}\sum_{i=1}^n(\frac{y_i-\hat{y}_i}{1-h_i})^2
   > $$
   > 其中$\hat{y}_i$为原始最小二乘拟合的第$i$个拟合值，$h_i$为杠杆值

3. **k折交叉验证法（k-fold CV）**，将观测值随机分为k个大小基本一致的组，或折（fold），选择一折作为验证集，剩下的k-1折作为训练集，重复k次，最后k折CV估计是k个测试误差估计的均值：$CV_{(k)}=\frac{1}{k}\sum_{i=1}^kMSE_i$，其原理如下图所示。

   <img src="images\ch5\5-3.png" style="zoom:50%;" />

   在实践中，一般令k=5或k=10，特别的，当k等于n时，该方法即为LOOCV。k折交叉验证法具有较多优点，最重要的是**该方法能兼顾到方差和偏差的权衡**。

### 5.1.2 自助法（Bootstrap）

&emsp;&emsp;自助法简单的来说就是对现有观测集进行有放回抽样，形成若干个样本，该原理如下图所示。

<img src="images\ch5\5-4.png" style="zoom:50%;" />

> 在Bootstrap中，用替换法从大小为N的数据集中采样N个实例，原始数据集被用作验证集。选择一个实例的概率是$\frac{1}{N}$;我们不选择这个实例的概率$1-\frac{1}{N}$。N次抽样后我们没有选中这个实例的概率是$(1-1/N)^{N} \approx e^{-1}=0.368$。这意味着训练数据只包含约$63.2\%$的实例，也就是说这个系统不会在另外$36.8\%$的数据上训练，这会为评估带来悲观偏差。解决方法是重复这个过程很多次来观察平均值。
>
> 转自：Introduction to Machine Learning（Ethem Alpaydin）

## 5.2 题目答案

1. 证明$\alpha = \frac {\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2 \sigma_{XY}}$使$Var(\alpha X + (1 - \alpha)Y)$最小。
   $$
   \begin{align}
   Var(\alpha X + (1 - \alpha)Y)
   &= Var(\alpha X) + Var((1 - \alpha) Y) + 2 Cov(\alpha X, (1 - \alpha) Y)
   \\
   &= \alpha^2 Var(X) + (1 - \alpha)^2 Var(Y) + 2 \alpha (1 - \alpha) Cov(X, Y)
   \\
   &= \sigma_X^2 \alpha^2 + \sigma_Y^2 (1 - \alpha)^2 + 2 \sigma_{XY} (-\alpha^2 +
   \alpha)\\
   &=(\sigma_X^2+\sigma_Y^2-2\sigma_{XY})\alpha^2+(2\sigma_{XY}-2\sigma_Y^2)\alpha+\sigma_Y^2\\
   \therefore argmin(Var(aX+(1-\alpha)Y))&=-\frac{2\sigma_{XY}-2\sigma_Y^2}{2(\sigma_X^2+\sigma_Y^2-2\sigma_{XY})}
   \end{align}
   $$
   
2. 见5.1.2
3. 4. 略

# 第六章 线性模型选择与正则化

&emsp;&emsp;本章讨论的是第3章中线性回归模型方法的拓展和改进，但其中涉及的概念同样适用于其他方法，例如第4章中介绍的分类模型。

## 6.1 相关概念

### 6.1.1 子集选择

1. **最优子集选择（best subset selection）**，即对p个预测变量的所有可能组合都建立模型，在所有模型中选择最优的一个，算法属于**暴力搜索法**，算法步骤如下：

   >**最优子集选择算法**
   >
   >* 记不含预测变量的零模型为$M_0$，只用于估计各观测的样本均值；
   >* 对于$k=1,2,...,p$：
   >  * 拟合$\binom{p}{k}$个包含$k$个预测变量的模型；
   >  * 在$\binom{p}{k}$个模型中选择RSS最小或$R^2$最大的作为最优模型，记为$M_k$。
   >* 根据交叉验证预测误差、$C_p(AIC)$、$BIC$或者调整$R^2$从$M_0,...,M_p$个模型中选择一个最优模型。

   该算法需要拟合$2^n$个模型，在模型的选取中，上述算法步骤II中，**将$2^n$个模型的最优选择转换为了从$p+1$个备选模型选择的问题**，即<u>先利用训练误差（RSS，$R^2$）在同等维度的模型选一个，再利用测试误差（或者可以是测试误差的无偏估计）在不同维度的模型中选一个</u>。

   该方法**可以选择最优的模型**，但是由于需要拟合$2^n$个模型，导致**计算效率低**。

2. **逐步选择（stepwise selection）**相比较于最优子集选择方法，该方法基于**贪婪思想**，选择后的模型逼近最优模型（很难找到最优模型），方法分为3种：**向前逐步选择（forward stepwise selection）**、**向后逐步选择（backward stepwise selection）**和**混合方法**。这类方法可以将模型拟合的次数转换为$\sum^{p-1}_{k=0}(p-k)=1+p(p+1)/2$。以上三次算法的大致思想在3.1.2.7中已经介绍，下面将总结这三类算法的具体步骤：

   > **向前逐步选择算法**
   >
   > * 记不含预测变量的零模型为$M_0$；
   > * 对于$k=0,1,2,...,p-1$：
   >   * 从$p-k$个模型中进行选择，每个模型都在模型$M_k$的基础上增加一个变量；
   >   * 在$p-k$个模型中选择RSS最小或$R^2$最高的模型作为最优模型，记为$M_{k+1}$。
   > * 根据交叉验证预测误差、$C_p(AIC)$、$BIC$或者调整$R^2$从$M_0,...,M_p$个模型中选择一个最优模型。

   > **向后逐步选择算法**
   >
   > * 记包含全部$p$个预测变量的全模型为$M_p$；
   > * 对于$k=p,p-1,...,1$：
   >   * 从$k$个模型中进行选择，在模型$M_k$的基础上减少一个变量，则模型只含$k-1$个变量；
   >   * 在$k$个模型中选择RSS最小或$R^2$最高的模型作为最优模型，记为$M_{k-1}$。
   > * 根据交叉验证预测误差、$C_p(AIC)$、$BIC$或者调整$R^2$从$M_0,...,M_p$个模型中选择一个最优模型。
   
   > **混合方法**
   >
   > 将向前和向后逐步选择进行结合，与向前逐步选择类似，该方法逐次将变量加入模型中，然而**在加入新变量的同时，该方法也移除不能提升模型拟合效果的变量**。

3. 在以上所有算法的最后一步都是基于**测试误差**的，因此RSS和$R^2$不适用。为了达到基于测试误差选择最优模型的目的，**需要估计测试误差**，通常有两种方法：

   * 根据过拟合导致的偏差对训练误差进行调整，**间接地估计测试误差**；
   * 通过验证集方法或交叉验证方法，**直接估计测试误差**。

4. 间接估计测试误差的方法主要有：$C_p$、**赤池信息量准则（Akaike information criterion, AIC）**、**贝叶斯信息准则（Bayesian information criterion, BIC）**与**调整$R^2$（adjusted $R^2$）**

   * $C_p=\frac{1}{n}(RSS+2d\hat{\sigma}^2)$，其中$\hat{\sigma}^2$是各个响应变量观测误差的方差$\sigma$的估计值，$C_p$统计量在训练集RSS的基础上增加惩罚项$2d\hat{\sigma}^2$，用于调整训练误差倾向于低估测试误差的现象。

     > Mallow的$C_p$定义为$C'_p=RSS/\hat{\sigma}^2+2d-n$，上式的$C_p$与$C'_p$等价，$C_p=\hat{\sigma}^2(C'_p+n)$，因此具有最小$C_p$值的模型也有最小的$C'_p$值

   * AIC准则适用于许多使用极大似然法进行拟合的模型，$AIC=\frac{1}{n\hat{\sigma}^2}(RSS+2d\hat{\sigma}^2)$，**AIC与$C_p$彼此成比例**。

   * BIC是从贝叶斯观点中衍生出来的，与$C_p$（及AIC）准则十分相似，$BIC=\frac{1}{n}(RSS+log(n)d\hat{\sigma}^2)$，**BIC统计量通常给包含多个变量的模型施以较重的惩罚，故而与$C_p$相比，得到的模型规模更小**。

   * **$R^2$随着模型包含的变量个数的增加而增加**，调整$R^2$可以避免这种情况，$调整R^2=1-\frac{RSS/(n-d-1)}{TSS/(n-1)}$，与其他准则不同，调整$R^2$的值越大，模型测试误差越低。

     > 调整$R^2$背后的想法是当模型包含了所有正确的变量，再增加其他冗余变量只会导致RSS小幅度的减小。在理论上，拥有最大调整$R^2$的模型只包含了正确的变量，而没有冗余变量，与$R^2$不同，调整后的$R^2$对纳入不必要变量的模型引入了惩罚。

### 6.1.2 压缩估计方法

1. **岭回归（ridge regression）**与最小二乘十分相似，该方法添加了**L2正则项**，即最小化下式：
   $$
   \sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_{ij})^2+\lambda\sum^p_{j=1}\beta_j^2=RSS+\lambda\sum^p_{j=1}\beta_j^2
   $$
   其中，$\lambda\geq0$为调节参数，需要使用交叉验证选取最优值，$\lambda\sum^p_{j=1}\beta_j^2$为L2正则项，称为压缩惩罚。**岭回归可以将回归系数的估计值压缩至零，$\lambda$越大，压缩比例越大（不对截距系数（偏置）压缩）。**其压缩结果如下图所示，当$\lambda$越来越大时，各系数趋于0.

   <img src="images\ch6\6-1.png" style="zoom:50%;" />

   > 注：岭回归与普通线性回归不同，后者无论第$j$个变量如何按比例变化，$X_j\hat{\beta}_j$保持不变，而岭回归$X_j\hat{\beta}_j$不仅取决于$\lambda$，还取决于第$j$个预测变量的尺度，甚至受其他预测变量尺度影响，因此在**做岭回归之前，需要对各个预测变量标准化**。

   **优：**与最小二乘法相比，岭回归综合权衡了误差与方差。**缺：**面对对模型没有影响（贡献）的变量，岭回归不会将它们剔除，仅仅将其系数向0方向缩减，所以岭回归最后的**模型仍然是一个全模型**。

2. **Lasso（least absolute shrinkage and selection operator）**

   与岭回归相比，Lasso基于**L1正则项**，在将回归系数压缩的同时，也对无用（无贡献）的预测变量进行剔除，Lasso的系数需要最小化下式：
   $$
   \sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_{ij})^2+\lambda\sum^p_{j=1}｜\beta_j｜=RSS+\lambda\sum^p_{j=1}｜\beta_j｜
   $$
   其中，$\lambda\sum^p_{j=1}｜\beta_j｜$称为L1正则项，当调节参数$\lambda$足够大时，L1惩罚项具有将其中某些系数的估计值强制设定为0的作用，得到了**稀疏模型（sparse model）**。**因此Lasso建立的模型比岭回归建立的模型相比更易于解释。**其压缩结果如下图所示，和岭回归一样，当$\lambda=0$时，与最小二乘等价，当$\lambda\rightarrow \infty$时，得到零模型。不同的是在两个极端之间，得到稀疏模型。

   <img src="images\ch6\6-2.png" style="zoom:50%;" />

3. **岭回归和Lasso对比**

   * Lasso和岭回归的系数估计等价于优化下列问题：
     $$
     \mathop{minimize}\limits_{\beta}\{\sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_{ij})^2\},\quad\sum_{j=1}^p|\beta_j|\leq s\\
     \mathop{minimize}\limits_{\beta}\{\sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_{ij})^2\},\quad\sum_{j=1}^p\beta_j^2\leq s
     $$
     以$p=2$为例，上式分别表示在一个菱形区域和圆形区域的约束下进行参数估计，如下图所示。

     <img src="images\ch6\6-4.png" style="zoom:35%;" />

     上图也解释了为什么Lasso既可以压缩系数，也可以选择变量子集，而岭回归只可以压缩系数，这是因为**岭回归的条件区域是没有尖点的圆形，所以相交点一般不会出现在坐标轴上**，而**Lasso的条件区域在每个坐标轴上都有拐角，所以椭圆经常在坐标轴上与条件区域相交**。

   * Lasso较岭回归具有较大优势，但两者那个精度高，或者效果好呢？根据**没有免费的午餐**定律，需要基于不同情形讨论，简单的来说：**当所有预测变量都对响应变量有贡献时，岭回归效果较好**；**当只有部分预测变量对响应变量有贡献时，Lasso效果较好**。

   * 两类方法比较于最小二乘时，如下图所示，**岭回归以相同比例 压缩每个维度**，而**Lasso中每个系数以$\lambda/2$为阈值压缩至零，即绝对值小于$\lambda/2$的系数被压缩至零，其他系数按比例压缩**。

     <img src="images\ch6\6-5.png" style="zoom:50%;" />

### 6.1.3 降维方法

1. 降维通过将预测变量进行转换，即将$X_1,...,X_p$变换成$Z_1,...,Z_M$，其中$Z_m$表示原始预测变量的线性组合（$M<p$），如下式所示：
   $$
   Z_m=\sum^p_{j=1}\phi_{jm}X_j
   $$
   其中，$\phi_{jm}$是常数。可以通过这些变换后的变量拟合模型。

2. **主成分回归（principal components regression，PCR）**基于**主成分分析（principal components analysis，PCA）**算法对各预测变量进行降维处理，是一种**无监督式**的方法。主成分回归遵循**将预测变量投影后观测值尽可能分散且各主成分互相正交**的原则，以第一主成分为例，该方向上数据的波动性最大，即投影的方差最大，而投影到其他方向的方差比这个小，如下图所示。

   <img src="images\ch6\6-6.png" style="zoom:50%;" />

   由于**少数的主成分足以解释大部分的数据波动和数据与响应变量之间的关系**，如果$Y$的方向就是$X_1,...,X_p$变动最为剧烈的方向，那么用$Z_1,...,Z_M$拟合一个最小二乘模型的结果要优于用$X_1,...,X_p$拟合的结果，估计$M\ll p$个系数会**减轻过拟合**。**主成分回归与岭回归非常相似，岭回归可以认为是连续型的主成分回归，因此主成分回归不适用于只包含特征变量子集的模型。**

   > 注：
   >
   > * 主成分数量一般通过交叉验证确定；
   >
   > * 在构造主成分前，需要对各变量做标准化处理，因为方差较大的变量将在主成分中占主导地位。
   >
   > * PCR的降维过程中由于没有响应变量的参与，因此无法保证那些很好地解释预测变量的方向同样可以很好预测响应变量。
   >
   > PCA原理将在第10章进行详细介绍，感兴趣可以提前参考下面的博客，讲得非常细致入微：http://blog.codinglabs.org/articles/pca-tutorial.html

3. **偏最小二乘（partial least squares，PLS）**是一种监督式的主成分回归替代方法，同PCR一样，PLS是一种**降维**手段。以第一个偏最小二乘方向$Z_1$为例，偏最小二乘法将$\phi_{j1}$设定为$Y$对$X_j$**简单线性回归的系数**，因此偏最小二乘法将最大权重赋给与响应变量相关性最强的变量。为确定第二个偏最小二乘方向，需对$Z_1$每个变量做回归，计算残差，再根据正交性计算$Z_2$，往后的最小二乘方向同理。

   PLS方向不仅可以像PCA一样很好地拟合预测变量，而且更好地解释了响应变量。同PCA一样，PLS的方向个数需要通过交叉验证选择，PLS虽然可以减小偏差，但可能同时增大方差，所以PLS和PCA需要的适用性分情况讨论。

### 6.1.4 高维问题

1. 大部分用于回归和分类的传统统计方法是在低维情况下发展而来，也就是观测数$n$远远大于特征数$p$。**当$p\geq n$即特征数比观测数大时，该数据被称为高维数据**，此时像最小二乘法这样传统的方法已不再适用。

   > 注：$p$可以很大，但是观测数$n$常常由于成本、抽样能力和其他因素受到限制。

2. 在高维数据的建模中，最常出现的问题就是**过拟合**。<u>以最小二乘法为例，不管特征变量和响应变量是否真正存在关系，最小二乘估计的系数都能很好地拟合数据，使得模型的残差为零，</u>如下图所示。

   <img src="images\ch6\6-7.png" style="zoom:40%;" />

   当特征变量变多时，即使这些变量与响应变量不相关，但**模型的$R^2$随着预测变量个数的增加而增长到1**，**相应的训练数据集均方误差降为0**，**独立测试集上的均方误差将变得非常大**，如下图所示。

   <img src="images\ch6\6-8.png" style="zoom:50%;" />

   **由于训练均方误差为0，$\hat{\sigma}^2=0$，$C_p$、AIC和BIC将不再适用。**

3. 由于上述高维数据导致的问题，传统回归分析已不再适用，本章介绍的**向前逐步选择、岭回归、lasso和主成分回归等将发挥重大作用**，这些回归算法避免了过拟合问题。在高维数据的建模中，有三个注意点：

   * 正则或压缩在高维问题中至关重要；
   * 合适的调节参数对于得到好的预测结果非常关键；
   * 测试误差随着数据维度（即特征或预测变量的个数）的增加而增大，除非新增的特征变量与响应变量缺失有关。（**维数灾难**）

   > 那些允许模型引入成千上万个特征变量的新方法就是一把**双刃剑**：如果特征变量与问题确实相关，那么它们将提升预测模型，否则将导致更糟的结果。

4. 在高维情况下，存在非常极端的**共线性**：模型中的任何一个变量都可以写成其他变量的线性组合。

5. 当$p>n$时容易得到一个残差为零蛋没有用的模型，因此**不能在训练集上用误差平方、p值、$R^2$统计量或者其他传统的度量方法来检测拟合的效果**，取而代之，更为重要的是**在独立的测试集上验证或者进行交叉验证**。

## 6.2 题目答案

1. 在单个数据集上使用最优子集选择、向前逐步选择和向后逐步选择。对于每种方法，可获得$p+1$个模型，每个模型包含$0,1,2,...,p$个预测变量。

   * 含有$k$个预测变量的三个模型中，**最优子集将具有最小的训练RSS**，因为模型将在训练RSS上进行优化，并且最佳子集已经PK掉了正向和反向选择将尝试的每个模型。
   * 含有$k$个预测变量的三个模型中，**最好的测试RSS模型可以是这三种模型中的任何一种**。如果数据相对于n个观测值具有较大的p个预测变量，则最优子集可能容易过拟合。向前和向后选择可能不会收敛于同一模型，但会尝试使用相同数量的模型，并且很难说哪种选择过程会更好。
   * 使用向前/后逐步选择法选取的$k$变量模型中的预测变量是使用向前/后逐步选择法选取的（$k+1$）变量模型中预测变量的子集。

2. * 与最小二乘相比，lasso和岭回归**灵活性（flexible）更差**，并且当模型预测结果的偏差增大的大小小于其方差减小的大小时，模型给出的预测值更准确。
   * 与最小二乘相比，非线性**灵活性（flexible）更好**，并且当模型预测结果的方差增大的大小小于其偏差减小的大小时，模型给出的预测值更准确。

3. 假设通过最小化以下式子来估计一个线性回归模型中的回归系数：
   $$
   \sum^n_{i=1}(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij}),\quad \sum_{j=1}^p|\beta_j|\leq s
   $$
   s为某给定值，下面的几个定论均为正确：

   * 随着s从0开始增加，训练集RSS会稳定减小。（s增加时，代表约束减小，模型使用的特征变多，进而容易过拟合）
   * 随着s从0开始增加，测试集RSS会最初减小，然后开始增加，图像呈现一个U形。（s=0代表刚开始是个零模型，模型处于欠拟合，然后逐渐转为过拟合，偏差减小，方差变大）
   * 随着s从0开始增加，方差稳定增长。（同训练集RSS，处于过拟合）
   * 随着s从0开始增加，偏差稳定减小。（同上，处于过拟合）
   * 随着s从0开始增加，不可约误差保持不变。（不可约误差为常数，不被模型影响）

4. 同3，略

5-7. 略（不会）

# 第七章 非线性模型

&emsp;&emsp;模型的线性假设通常只是对真实函数的一种近似，有时拟合效果并不理想（大部分都不理想……），虽然lasso、岭回归等算法已经对线性模型做出了优化，但是还是没有改变其线性模型的形式。在保证模型良好的解释性的前提下，放松线性假设会大大提高预测的准确度。

## 7.1 相关概念

### 7.1.1 多项式回归

&emsp;&emsp;多项式回归是线性模型的一个非线性推广，是一个全局的模型，形式为：
$$
y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+...+\beta_dx_i^d+\epsilon_i
$$
其中，$\epsilon_i$是误差项。上式也可看成**预测变量$x_i,\ x_i^2,\ x_i^3,\ ...,\ x_i^d$的标准线性模型，因此可以使用最小二乘法进行参数估计**。

> 注：多项式阶数$d$的选择不宜过大，**一般不大于3或4**，当$d$越大时，多项式曲线就会变得越光滑，在$X$变量定义域边界处呈现异样形状。

### 7.1.2 阶梯函数

&emsp;&emsp;阶梯函数非全局，该模型**将$X$的取值范围分为一些区间**，**每个区间拟合一个不同的常数**。具体地，在$X$取值空间上创建分割点$c_1,\ c_2,\ ...,\ c_K$，然后构造$K+1$个新变量：
$$
\begin{align}
C_0(X)&=I(X<c_1)\\
C_1(X)&=I(c_1\leq X<c_2)\\
C_2(X)&=I(c_2\leq X<c_3)\\
& \vdots\\
C_{K-1}(X)&=I(c_{K-1}\leq X<c_K)\\
C_K(X)&=I(c_K\leq X)
\end{align}
$$
其中，$I(\cdot)$是示性函数，当条件成立时返回1否则返回0.因为$X$只能落在一个区间内，所以$C_0(X)+C_1(X)+...+C_K(X)=1$，以该变量作为预测变量，构建模型：
$$
y_i=\beta_0+\beta_1C_1(x_i)+\beta_2C_2(x_i )+...+\beta_KC_K(x_i)+\epsilon_i
$$

> 注：当$X<c_1$时，上式每个预测变量都为零，所以$\beta_0$为$X<c_1$时$Y$的平均值。相应的，当$c_j<X<c_{j+1}$时，预测值为$\beta_0+\beta_j$，这样$\beta_j$则被解释为当$X$由$X<c_1$增至$c_j<X<c_{j+1}$时响应变量的平均增量。

&emsp;&emsp;如果预测变量不具有明显的分割点，那么分段固定值拟合就十分不恰当，如下图所示。

<img src="images\ch7\7-1.png" style="zoom:50%;" />

### 7.1.3 基函数

&emsp;&emsp;多项式和阶梯函数实际上都是特殊的基函数，基函数原理是对变量$X$的函数或变换$b_1(X),\ b_2(X),\ ...,\ b_k(X)$进行建模，形式为：
$$
y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i )+...+\beta_Kb_K(x_i)+\epsilon_i
$$

### 7.1.4 回归样条

1. 样条函数是多项式回归和阶梯函数回归的延伸和推广，即**在$X$的不同区域拟合独立的低阶多项式函数**，以此取代$X$在全部取值范围内拟合高阶多项式。系数发生变化的临界点称为**结点（knot）**。以分段三次多项式为例，其在不同区域的拟合形式为：
   $$
   y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+\epsilon_i
   $$
   当结点数为1时，模型形式为：
   $$
   y_i=
   \left\{\begin{aligned}
   \beta_{01}+\beta_{11}x_i+\beta_{21}x_i^2+\beta_{31}x_i^3+\epsilon_i,\quad x_i<c\\
   \beta_{02}+\beta_{12}x_i+\beta_{22}x_i^2+\beta_{32}x_i^3+\epsilon_i,\quad x_i\geq c
   \end{aligned}\right.\\
   $$
   即模型拟合两个不同的多项式函数，其中有个在$x_i<c$的子集上，另一个在$x_i\geq c$的子集上。**由于每个模型有四个参数，所以共使用了8个自由度构建该分段多项式模型**。

2. 由于结点的存在，如果对**结点部分不加以约束**，将会出现下图左上角情况，即函数不连续。当添加了**确保拟合曲线是连续**的约束条件，图像则是右上角情况，但仍会出现尖点。当在**结点处添加一阶导数和二阶导数都是连续**的约束条件，如左下角情况，结点处是光滑的。值得注意的是，**每个对分段三次多项式施加的约束都有效地释放了一个自由度**，减少模型的复杂性。如左下图施加了三个约束（连续性、一阶导数的连续性、二阶导数的连续性），将有5个约束，一般情况下**$K$个结点的三次样条会产生$4+K$个自由度**。

   <img src="images\ch7\7-2.png" style="zoom:50%;" />

3. 上述的回归样条略显复杂，需要保证多项式自身以及其前$d-1$阶导数是连续的约束，可以基于基函数模型来表示这样的回归样条，选择合适的基函数表示成如下形式：
   $$
   y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i )+...+\beta_{K+3}b_{K+3}(x_i)+\epsilon_i
   $$
   对于每个结点可以添加**截断幂基**函数，其形式为：
   $$
   h(x,\xi)=(x-\xi)^3_+=\left\{\begin{aligned}
   &(x-\xi)^3 \quad &x>\xi\\
   &0\quad &否则
   \end{aligned}\right.\\
   $$
   其中，$\xi$是结点。

4. 若对边界不加以约束，其拟合曲线的置信带将相当宽，**自然样条（natural spline）**对边界处添加了约束条件：函数在边界区域应该是线性的（边界区域指的是比最小结点处左侧和最大结点的右侧），如下图红色部分所示，红色的置信区间更窄。

   <img src="images\ch7\7-3.png" style="zoom:50%;" />

5. 样条函数的结点选择不当将引起曲线跌宕起伏，实践证明，**令结点在数据上呈现均匀分布是一种比较有效的结点选择方式**，**前提是需要确定所需的自由度**。**选择结点数量客观的方法是交叉验证**。样条函数可以在函数$f$变动较快的区域设置结点，在$f$稳定的地方设置较少结点，保证曲线的光滑性。

### 7.1.5 光滑样条

1. 与前面的样条函数不同，光滑样条不需要先设定一些结点，较为特殊的，光滑样条将**所有不同的$x_i$都设为结点**，然后再**调整样条函数的光滑度**，其优化目标为最小化下式：
   $$
   \sum^n_{i=1}(y_i-g(x_i))^2+\lambda\int g''(t)^2dt
   $$
   其中，$\lambda$是一个非负的调节参数，上式的形式为**“损失函数+惩罚项”**，形如岭回归以及lasso，但不同的是，光滑样条的惩罚项是**控制模型光滑度（或者是粗糙度，roughness）**的，因为函数的一阶导衡量斜率，二阶导衡量斜率的变化程度，即粗糙度。当$\lambda=0$时，惩罚项不起作用，函数$g$将在训练数据上做插值，造成过拟合；当$\lambda\rightarrow \infty$时，$g$变得非常平稳，曲线将变为一条直线，实际上是一条最小二乘直线；当$\lambda$适中时，$g$将尽可能地接近训练点同时也比较光滑。

2. 在光滑样条中，每个数据点都作为一个结点将导致自由度太高，因为$\lambda$允许从0增加到$\infty$，实际的自由度（记作$df_\lambda$，也称**有效自由度**）就从n降到2.自由度指的是自由参数的个数，尽管光滑样条有$n$个参数，即名义上的自由度为$n$，**但$n$个自由度被大量限制或者收缩**。有效自由度计算如下：
   $$
   \pmb{\hat{g}}_\lambda=\pmb{S}_\lambda \pmb{y}\\
   df_\lambda=\sum_{i=1}^n\{\pmb{S}_\lambda\}_{ii}
   $$
   其中，$\pmb{\hat{g}}$是在特定$\lambda$下，光滑样条的拟合值向量，上式表明光滑样条的拟合值向量可以写成$n\times n$的矩阵$\pmb{S}_\lambda$乘以响应向量$y$。有效自由度是矩阵$S_\lambda$的对角元素之和。

3. 虽然不需要事先选择结点个数或位置，但需要确定$\lambda$的值，常使用交叉验证法。特别的，当使用留一交叉验证时（LOOCV），代价与与计算一个拟合模型一样：
   $$
   RSS_{cv}(\lambda)=\sum_{i=1}^n(y_i-\hat{g}_\lambda^{(-i)}(x_i))^2=\sum_{i=1}^n[\frac{y_i-\hat{g}_\lambda(x_i)}{1-\{\pmb{S}_\lambda\}_{ii}}]^2
   $$
   其中，$\hat{g}_\lambda^{(-i)}$是剔除观测点$(x_i, y_i)$外的所有训练数据的拟合值。上式的特别之处：**当计算每个点的留一交叉验证结果时，相当于计算每一个点用所有数据的原始拟合值$\hat{g}_\lambda$.**

### 7.1.6 局部回归

&emsp;&emsp;局部回归在对一个目标观测点$x_0$拟合时，**只用到该点附近的训练观测**，如下图所示，蓝色为真实函数$f(x)$的曲线，浅橘黄色使用局部回归拟合得到的$\hat{f}(x)$的曲线。橘黄色的点表示的是橘黄色直线指示的点$x_0$的邻居，黄色钟形区域代表赋值给$x_0$的邻居的权重，**权重随着距$x_0$的距离越远降至微0**.即**$x_0$的拟合值$\hat{f}(x_0)$是通过拟合一个加权线性回归得到的**。

<img src="images\ch7\7-4.png" style="zoom:50%;" />

>**在$X=x_0$处的局部回归模型算法**
>
>1. 选取占所有数据$s=k/n$比例的最靠近$x_0$的数据$x_i$；
>
>2. 对选出的数据点赋予其权重$K_{i0}=K(x_i,x_0)$。**离$x_0$最远的点的权重为0，而最近的点权重最高**。那些没有被选中的数据点的权重为0；
>
>3. 用定义好的权重在$x_i$处拟合加权最小二乘回归，也就是对下式最小化
> $$
>   \sum^n_{i=1}K_{i0}(y_i-\beta_0-\beta_1x_i)^2
> $$
>
>4. 根据$\hat{f}(x_0)=\hat{\beta}_0+\hat{\beta}_1x_0$得到$x_0$的拟合结果。

> 局部回归很复杂，比如如何定义权重函数$K$，第3步应该使用什么函数，第1步中间距$s$的定义等。**地理加权回归（GWR）**是该算法的拓展，因此将在后面空间统计的学习加以详细介绍，在此也立个Flag🚩，<u>后期将做一个空间统计的学习笔记</u>。

### 7.1.7 广义可加模型

1. **广义可加模型（generalized additive model，GAM）**基于多个变量$X_1,\ ...,\ X_n$，提供了一种对标准线性模型进行推广的框架，**每一个变量用一个非线性函数替换**，GAM既可以用于响应变量是定性的情形，也可以用于响应变量是定量的情形。

2. 用于回归问题的GAM，该模型可以将多元线性回归模型重新写成如下形式：
   $$
   \begin{align}
   y_i&=\beta_0+\sum^p_{j=1}f_j(x_{ij})+\epsilon_i\\
   &= \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + ... + f_p(x_{ip}) + \epsilon_i
   \end{align}
   $$
   其中，$f_j(x_{ij})$为光滑的非线性函数，是一个**独立的计算单元**。前面介绍的几个模型都可以作为计算单元。

3. 用于分类问题的GAM，该模型基于逻辑斯蒂回归模型，改写成如下形式：
   $$
   log(\frac{p(X)}{1-p(X)})=\beta_0 + f_1(X_1) + f_2(X_2) + ... + f_p(X_p)
   $$

4. GAM的**优点**：

   * GAM可以对每个变量拟合一个非线性函数，避免手动设置每个变量的变形方式；
   * 非线性拟合模型能将响应变量预测得更准；
   * 由于模型可加，可以固定其他变量，看一个变量对响应变量的影响效果，**适合推断**；
   * 针对变量$X_j$的函数$f_j$的光滑性可以通过对自由度的分析得到；

   GAM的**局限**：

   * 模型的形式被限定为可加形式，在多变量的情况下，这类模型会忽略有意义的交互项。需要添加形式为$X_j \times X_k$的交互项式的GAM也能够表达交互效应。

## 7.2 题目答案

1. (a). $a_1 = \beta_0, b_1 = \beta_1, c_1 = \beta_2, d_1 = \beta_3$.

   (b). 
   $$
   \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x - \xi)^3
   \\
   = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x^3 - 3 x^2 \xi + 3 x \xi^2 - \xi^3)
   \\
   = (\beta_0 - \beta_4 \xi^3) + (\beta_1 + 3 \beta_4 \xi^2) x + (\beta_2 - 3 \beta_4 \xi) x^2 + (\beta_3 + \beta_4) x^3\\
   
   \therefore a_2 = \beta_0 - \beta_4 \xi^3, b_2 = \beta_1 + 3 \beta_4 \xi^2, c_2 = \beta_2 - 3 \beta_4 \xi, d_2 = \beta_3 + \beta_4
   $$
   (c).
   $$
   f_1(\xi) = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3
   \\
   f_2(\xi) = (\beta_0 - \beta_4 \xi^3) + (\beta_1 + 3 \beta_4 \xi^2) \xi + (\beta_2 - 3 \beta_4 \xi) \xi^2 + (\beta_3 + \beta_4) \xi^3
   \\
   = \beta_0 - \beta_4 \xi^3 + \beta_1 \xi + 3 \beta_4 \xi^3 + \beta_2 \xi^2 - 3 \beta_4 \xi^3 + \beta_3 \xi^3 + \beta_4 \xi^3
   \\
   = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + 3 \beta_4 \xi^3 - 3 \beta_4 \xi^3 + \beta_3 \xi^3 + \beta_4 \xi^3 - \beta_4 \xi^3
   \\
   = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3
   $$
   (d).
   $$
   f'(x) = b_1 + 2 c_1 x + 3 d_1 x^2 
   \\
   f_1'(\xi) = \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2
   \\
   f_2'(\xi) = \beta_1 + 3 \beta_4 \xi^2 + 2 (\beta_2 - 3 \beta_4 \xi) \xi + 3 (\beta_3 + \beta_4) \xi^2
   \\
   = \beta_1 + 3 \beta_4 \xi^2 + 2 \beta_2 \xi - 6 \beta_4 \xi^2 + 3 \beta_3 \xi^2 + 3 \beta_4 \xi^2
   \\
   = \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2 + 3 \beta_4 \xi^2 + 3 \beta_4 \xi^2 - 6 \beta_4 \xi^2 
   \\
   = \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2
   $$
   (e). 
   $$
   f''(x) = 2 c_1 + 6 d_1 x
   \\
   f_1''(\xi) = 2 \beta_2 + 6 \beta_3 \xi
   \\
   f_2''(\xi) = 2 (\beta_2 - 3 \beta_4 \xi) + 6 (\beta_3 + \beta_4) \xi
   \\
   = 2 \beta_2 + 6 \beta_3 \xi
   $$

2. 假设曲线$\hat{g}$是由式子$\hat{g}=\arg\ \mathop{\min}\limits_{g}(\sum^n_{i=1}(y_i-g(x_i))^2+\lambda\int[g^{(m)}(x)]^2dx)$光滑拟合$n$个点得到的，其中$g^{(m)}$是$g$的第$m$阶导数（$g^{(0)}=g$）。请举例说明如下情形下的$\hat{g}$。

   (a).$\lambda=\infty,m=0$

   左边项将没有意义，$\hat{g}=0$，恒为0

   (b).$\lambda=\infty,m=1$

   左边项将没有意义，$\hat{g}=k$，为常数

   (c).$\lambda=\infty,m=2$

   左边项将没有意义，$\hat{g}=ax+b$，为直线

   (d).$\lambda=\infty,m=3$

   左边项将没有意义，$\hat{g}=ax^2+bx+c$，为二次曲线

3. 略

4. 略

5. 考虑通过以下式子定义的两条曲线$\hat{g}_1$，$\hat{g}_2$
   $$
   \hat{g}_1=\arg\ \mathop{\min}\limits_{g}(\sum^n_{i=1}(y_i-g(x_i))^2+\lambda\int[g^{(3)}(x)]^2dx)\\
   \hat{g}_2=\arg\ \mathop{\min}\limits_{g}(\sum^n_{i=1}(y_i-g(x_i))^2+\lambda\int[g^{(4)}(x)]^2dx)
   $$
   其中$g^{(m)}$表示$g$的第$m$阶导数。

   (a)当$\lambda\rightarrow\infty$时，由于$\hat{g}_1$比$\hat{g}_2$的条件更严格，因此$\hat{g}_2$更光滑，所以$\hat{g}_2$的训练RSS更小；

   (b)当$\lambda\rightarrow\infty$时，因为不知道真实的曲线，所以无法评价两者的测试RSS哪个小；

   (c)当$\lambda=0$时，因为惩罚项不起作用，所以$\hat{g}_1$和$\hat{g}_2$等价。

# 第八章 基于树的方法

&emsp;&emsp;相比较于前面的回归和分类算法，基于树的方法根据**分层**和**分割**的方式将预测变量空间划分为一系列的简单区域，并用区域内的统计量进行预测。

## 8.1 相关概念

### 8.1.1 决策树

1. 决策树可以看中一些列if...else...的集合，**其结构**与自然界中的树类似，有**终端结点（terminal node）**或**树叶（leaf）**、**内部结点（internal node，可以看成是分类的条件）**以及连接部分，称为**分支（branch）**，决策树通常是从上到下绘制，树叶位于树的地步，如下图所示。

   <img src="images\ch8\8-1.png" style="zoom:50%;" />

2. **回归树（regression tree）**针对连续型（定量）的响应变量，可以分为以下两步：

   * 将预测变量空间（即$X_1,\ X_2,\ ...,\ X_p$的可能取值构成的集合）**分割成$J$个互不重叠的区域**$R_1,\ R_2,\ ...,\ R_J$，如下图所示，**其中左上图不能由递归二叉分裂决策树划分**。

     <img src="images\ch8\8-2.png" style="zoom:50%;" />

   * 对落入区域$R_j$的每个观测值作同样的预测，预测值等于$R_j$上训练集的响应值的**简单算术平均**。

   上述第一步中，将预测变量空间划分为**高维矩形**，或称为**盒子**。划分目标是找到使模型的RSS最小的区域$R_1,\ R_2,\ ...,\ R_J$，RSS定义为：
   $$
   RSS=\sum_{j=1}^J\sum_{i\in R_j}(y_i-\hat{y}_{R_j})^2
   $$
   其中，$\hat{y}_{R_j}$是第$j$个矩形区域中训练集的平均响应值。划分采用**自顶向下**、**贪婪**的方法：**递归二叉分裂（recursive binary splitting）**，即**不在乎全局最优，只关注每一步的最优分裂**。在执行该分裂时，先选择预测变量$X_j$和分割点$s$，将预测变量空间分为两个区域$\{X|X_j<s\}$和$\{X|X_j\geq s\}$，使RSS尽可能小。对每个分裂后的区域重复上述方法，直到**满足某个停止准则**。

3. **分类树（classification tree）**的过程与回归树一样，只是该算法针对的对象是定性响应变量，因此不能使用RSS作为确定分裂准则，**且给定观测值被预测为它所属区域内的训练集中催常出现的类**。该模型的分类准则常用三种，分别为：

   * **分类错误率（classification error rate）**，该指标是RSS非常自然的替代，**错误率定义为区域内训练集中非常见类所占的比例**，表达式如下：
     $$
     E=1-\max_k(\hat{p}_{mk})
     $$
     其中，$\hat{p}_{mk}$代表第$m$个区域的训练集中第$k$类所占比例。但是分类错误率仅对精度敏感，对构建树时的纯度不敏感，因此**当追求较高的预测准确性时适用该指标**。

   * **基尼系数（Gini index）**被视为衡量结点$纯度（purity）$的指标，**基尼系数较小时，意味着某个结点包含的观测值几乎都来自同一类**。基尼系数定义如下：
     $$
     G=\sum^K_{k=1}\hat{p}_{mk}(1-\hat{p}_{mk})
     $$
     其中，如果所有$\hat{p}_{mk}$的取值都接近于0或1，基尼系数会很小。

   * **交叉熵（cross entropy）**同基尼系数都衡量纯度，两者在数值上是相当接近的。其定义如下：
     $$
     D=-\sum^K_{k=1}\hat{p}_{mk}\log\hat{p}_{mk}
     $$
     其中，如果所有$\hat{p}_{mk}$的取值都接近于0或1，交叉熵会很小。

4. 决策树算法可以在训练集上得到很好的效果，但是容易造成过拟合，因此需要**剪枝（prune）**处理。剪枝的目的是选出使测试集预测误差最小的子树（子树相当于整个树$T_0$的子集）。常用**代价复杂性剪枝（cost complexity pruning）**，也称**最弱联系剪枝（weakest link pruning）**，<u>该方法不考虑每一颗可能的子树，而是考虑以非负调整参数$\alpha$标记的一系列子树</u>。每一个$\alpha$的取值对应一棵子树$T\in T_0$，当$\alpha$一定时，使下式最小：
   $$
   \sum^{|T|}_{m=1}\sum_{i:x_i\in R_m}(y_i-\hat{y}_{R_m})^2+\alpha|T|
   $$
   其中，$|T|$表示$T$的终端结点数，$\hat{y}_{R_m}$是与$R_m$对应的预测值。右侧项为惩罚项，当$\alpha=0$时，子树$T$等于原树$T_0$，因此上式只衡量了训练误差；当$\alpha$增大时，得到最小值的子树将会变得更小。需要使用交叉验证确定$\alpha$。

   > **建立决策树的算法步骤**
   >
   > * 利用递归二叉分裂在训练集中生成一棵大树，只有当终端结点包含的观测值个数低于某个最小值时才停止；
   >
   > * 对大树进行代价复杂性剪枝，得到一系列最优子树，子树是$\alpha$的函数；
   >
   > * 利用$K$折交叉验证选择$\alpha$：
   >
   >   * 对非验证集的数据重复步骤1和2，得到与$\alpha$一一对应的子树；
   >   * 求出上述子树在第k折（验证集）的RSS或基尼系数等。
   >
   >   上述步骤结束后，每个$\alpha$会有相应的$K$个分裂准则统计量，对其求平均，并选出使其值最小的$\alpha$。
   >
   > * 找出选定的$\alpha$在步骤2中对应的子树即可。

5. 与前面几章的传统方法相比，决策树有许多**优点**：

   * 解释性强，甚至比线性回归还方便；
   * 更接近人的决策模式（因为是if…else…的集合）；
   * 可以用图形表示，非专业人士也能轻松看懂；
   * 可以直接处理定性的预测变量而不需要创建哑变量。

   但其**缺点**是准确性无法达到其他回归和分类的方法。如果问线性模型和树模型哪个更好，应该视具体情况而定，如下图清晰地说明了原因。

   <img src="images\ch8\8-3.png" style="zoom:50%;" />

### 8.1.2 装袋法（Bagging）

&emsp;&emsp;单棵树具有低偏差、高方差，即容易过拟合的问题，如果建立多棵树（设为$n$棵），那么其**均值方差即为$\sigma^2/n$**（$\sigma^2$为单棵树的方差）。建立多棵树即需要不同的样本，这时**Bootstrap**将派上了大用场。对于**回归树**来说，抽取B个样本，并基于每个样本拟合出B个模型，**对多个预测值求平均**：
$$
\hat{f}_{bag}(x)=\frac{1}{B}\sum^B_{b=1}\hat{f}^b(x)
$$
分类树的建模思想大致如此，但是最后求预测值的方法不同，其采用**多数投票法（majority vote）**，即**将B个预测中出现频率最高的类作为总体预测**。

> 注：Bagging的每一棵树不需要剪枝，将偏差降到最低后，利用成百上千棵树拟合，可以降低方差。当树的数量B很大时，不会产生过拟合，反而能使误差稳定下来。

&emsp;&emsp;基于Bootstrap有一个天然的优势，即**2/3的数据会被抽到，1/3的数据始终不能被抽到**（证明见5.1.2），因此可以将1/3的观测值用于预测，其被称为树的**袋外（out-of-bag，OOB）**观测值。具体为，用所有将第$i$个观测值作为OOB的树来预测第$i$个观测值的响应值，便生成约B/3个对第$i$个观测值的预测，对这些预测取平均或多次投票，即可得到一个观测的OOB预测。

> 注：Bagging对预测准确性的提升是牺牲解释性为代价的。

### 8.1.3 随机森林（Random forest）

&emsp;&emsp;虽然Bagging看上去十分完美，但是实际上**观测值之间可能存在相关性**，导致Bagging的方差结果并不为$\sigma^2/n$，甚至一**直是$\sigma^2$，随机森林便针对该问题作了改进。随机森林在构建树时，**在每个分裂点处都会从所有（$p$个）预测变量中抽取$m$个变量用于分裂，其中$m\approx\sqrt{p}$，这个过程被称为**去相关（decorrelate）**。

> 举例：数据有100个特征，其中有5个高度相关，如果使用Bagging每次都会涵盖这5个高度相关的特征，而随机森林每次分裂时只选用10个（$\sqrt{100}$），抽到这5个高度相关的概率为$\frac{\binom{5}{5}\binom{5}{95}}{\binom{10}{100} }=0.00000335$，大大的降低了观测之间存在高度相关的可能。（具体推导不展开，将在《机器学习》和《统计学习方法》的笔记中展开）

### 8.1.4 提升法（Boosting）

&emsp;&emsp;Boosting和Bagging都是通用的方法，不仅限于决策树。Bagging和随机森林的树都是相互独立的，而Boosting的树是依次生成的，有一个逐步学习的过程，因此相比较于生成一个**大树意味着对数据的严格契合**并可能造成过拟合来说，Boosting是一种舒缓的方式，该方法对模型的残差$r_i=y_i-\hat{y}_i$建模。

> 提升法的步骤，以回归树为例
>
> 1. 对训练集中的所有$i$，令$\hat{f}(x)=0,r_i=y_i$；（即起初残差很大）
>
> 2. 对$b=1,2,...,B$重复一下步骤：
>
>    * 对训练数据（X～r），建立一棵有$d$个分裂点（$d+1$个终端结点）的树$\hat{f}^b$；
>
>    * 将压缩后的新树加入模型以更新$\hat{f}$：（预测值逐渐向真实值靠拢）
>      $$
>      \hat{f}(x)\leftarrow \hat{f}(x) + \lambda\hat{f}^b(x)
>      $$
>
>    * 更新残差：（残差越来越小）
>      $$
>      r_i \leftarrow r_i - \lambda\hat{f}^b(x_i)
>      $$
>
> 3. 输出经过提升的模型：
>    $$
>    \hat{f}(x)=\sum^B_{b=1}\lambda\hat{f}^b(x)
>    $$

> 上述方法可以以一个简单的例子描述：
>
> 目标：预测手上有10元钱，初始预测为0元，残差为10元
>
> step 1:针对10元的残差，模型预测了6元，还剩4元残差；
>
> step 2:针对4元的残差，模型预测了2元，还剩2元残差；
>
> step 3:针对2元的残差，模型预测了1元，还剩1元残差；
>
> step 4:针对1元的残差，模型预测了0.5元，还剩0.5元残差；
>
> 最终，总的预测值=0+6+2+1+0.5=9.5元

&emsp;&emsp;Boosting有三个调整参数：

* 树的总数B。与Bagging和随机森林不同，Boosting中B值越大越容易过拟合；
* 压缩参数$\lambda$。控制着Boosting的学习速度，通常取0.01或0.001；
* 每个树的分裂点数$d$，控制着模型的复杂度，当$d=1$时效果最好，此时每棵树都是个$树桩（stump）$，此时Boosting与加法模型相符。

## 8.2 题目答案

2. 用深度为1的树构建提升法会得到加法模型，形式如下：
   $$
   f(X)=\sum^p_{j=1}f_j(X_j)
   $$
   

   因当$d=1$时，仅依靠一个预测变量，然而需要用到所有预测变量，所以最后的模型是一个加法模型。

其他略（基本上都是画图题）

# 第九章 支持向量机

&emsp;&emsp;**支持向量机（support vector machine，SVM）**是20世纪90年代在计算机界发展起来的一种分类方法，自推出后非常受欢迎，该算法的设计非常巧妙，与之前介绍的几种算法大不相同（和逻辑斯蒂回归有紧密的联系）。以下将介绍最大间隔分类器、支持向量分类器和支持向量机，上述三个算法都简单地叫做“支持向量机”。

> 注：SVM的算法细节很复杂，在该笔记中不做详细推导，该算法推导以及应用将在后续算法专题的博客中加以详细介绍。

## 9.1 相关概念

### 9.1.1 最大间隔分类器

1. **超平面（hyperplane）**是支持向量机系列算法里中的一个重要概念，其定义为**$p$维空间中的$p-1$维的平面仿射子空间**，例如二维空间的超平面是一维子空间（直线），三维空间的超平面是二维子空间（平面）。$p$维超平面的形式如下：
   $$
   \beta_0 + \beta_1X_1 + \beta_2X_2 +...+\beta_pX_p = 0
   $$
   满足上式条件的点$X=(X_1, X_2, ..., X_p)^T$（长度为$p$的向量）则称该点**落在超平面**上；若该点的左项值大于0或小于0，说明点**落在超平面的一侧**，根据**该值的符号确定该数据点的类别**，如果**值距离零很远，代表数据点离超平面很远**。下图为2维空间的一个超平面。

   <img src="images\ch9\9-1.png" style="zoom:40%;" />

2. 能将数据分开的超平面有无数个，但最优的超平面是离数据点最远的，也称为**最大间隔超平面（maximal margin hyperplane）**或**最优分离超平面（optimal separating hyperplane）**。具体为计算每个训练观测到一个特定分割超平面的（垂直）距离，这些距离的最小值就是训练观测与分割超平面的距离，称为**间隔（margin）**。通过最大间隔超平面来判断数据的类别，即为**最大间隔分类器（maximal margin classifier）**，当数据特征数量$p$过大时，最大间隔分类器容易出现过拟合问题。

3. 最大间隔超平面就是两个类别之间最宽的“平板”的中线，如下图所示。虚线表示了间隔宽度，落在虚线上的三个点称为**支持向量（support vector）**，它们“支持”了最大间隔超平面，只要这三个点**位置改变**，最大间隔超平面也会**随之移动**，所以**最大间隔超平面只与支持向量有关，跟其他的观测无关**。

   <img src="images\ch9\9-2.png" style="zoom:40%;" />

4. 考虑$n$个训练观测$x_1,x_2,...,x_n\in \mathbf{R}^p$和类别标签$y_1,y_2,...,y_n\in{-1,1}$，最大间隔超平面即为如下优化问题的解：
   $$
   \begin{align}
   &\max_{\beta_0,\beta_1,...,\beta_p}M\\
   s.t. \\&\sum^p_{j=1}\beta_j^2=1\\
   &y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_px_{ip})\geq M\\
   \end{align}
   $$
   第二个约束条件（$y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_px_{ip})\geq M$）要求每个观测和超平面之间保持一定的距离，第一个约束条件（$\sum^p_{j=1}\beta_j^2=1$）使得第$i$个观测到超平面的垂直距离为$y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_px_{ip})$，所以$M$代表了超平面的间隔。

> 注：在线性不可分情况中，最大间隔分类器不适用。

### 9.1.2 支持向量分类器

1. **支持向量分类器（support vector classifier）**也称为**软间隔分类器（soft margin classifier）**，是最大间隔分类器在线性不可分时的推广，该算法对单个观测的变化不敏感，**允许小部分训练观测误分**，不考虑完美分类的超平面分类器，这也是“软”间隔的原因。分类示意如下图所示。

   <img src="images\ch9\9-4.png" style="zoom:50%;" />

2. 支持向量分类器的求解方法同最大间隔分类器，但多了一个约束条件，如下式：
   $$
   \begin{align}
   &\max_{\beta_0,\beta_1,...,\beta_p,\epsilon_1,\epsilon_2,...,\epsilon_n}M\\
   s.t. \\&\sum^p_{j=1}\beta_j^2=1\\
   &y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+...+\beta_px_{ip})\geq M\\
   &\epsilon_i\geq0,\sum^n_{i=1}\epsilon_i\leq C
   \end{align}
   $$
   其中，$\epsilon_1,...,\epsilon_n$是**松弛变量（slack variable）**，松弛变量的作用是**允许观测落在间隔的错误一侧，或超平面错误的一侧**；$C$是非负的调节参数，是所有$\epsilon_i$之和的上界，**表示容忍穿过间隔（或超平面）的观测的数目和严重程度**。$\epsilon_i$的规律如下：

   * 如果$\epsilon_i=0$，那么第$i$个观测就落在间隔的正确一侧；
   * 如果$\epsilon_i>0$，那么第$i$个观测就落在间隔的错误一侧，即第$i$个观测穿过了间隔；
   * 如果$\epsilon_i<0$，那么第$i$个观测就落在超平面的错误一侧。

   $C$的规律如下：

   * 如果$C=0$，既不存在穿过间隔的观测，同时$\epsilon_i=0$必成立，等价于最大间隔超平面；
   * 如果$C>0$，即只有不超过$C$个观测可以落入超平面错误的一侧。

   **随着$C$的增加，越来越容许观测穿过间隔，间隔越来越宽；相反，不容许观测穿过间隔，从而间隔变窄。**<u>该参数的调节需要交叉验证。</u>

3. 在支持向量分类器中，刚好**落在间隔上和落在间隔的错误一侧**的观测叫做支持向量。当$C$比较大时，间隔较宽，穿过间隔的观测比较多，该分类器具有**较低的方差**，但可能有**较高的偏差**；当$C$比较小时，间隔较窄，穿过间隔的观测比较少，该分类器具有**较高的方差**，但可能有**较低的偏差**。

4. 不加证明地介绍，支持向量分类器可以描述为
   $$
   f(\mathbf{x})=\beta_0 + \sum^n_{i=1}\alpha_i<\mathbf{x},\mathbf{x}_i>
   $$
   其中，$\mathbf{x}$为需要计算的观测向量，$\mathbf{x}_i$为训练观测向量，$<\mathbf{x},\mathbf{x}_i>$为向量的内积（点积），$\alpha_i$为每个训练观测对应的参数。为了估计参数，需要训练观测的$\binom{n}{2}$个成对组合的内积。若将支持向量对应的参数$\alpha_i$设为非零，而非支持向量对应的参数$\alpha_i$设为零，能大大减小计算开销，用$S$代表支持向量观测点的指标的集合：
   $$
   f(\mathbf{x})=\beta_0 + \sum_{i\in S}\alpha_i<\mathbf{x},\mathbf{x}_i>
   $$

> 支持向量分类器是线性的。

### 9.1.3 狭义的支持向量机

1. 非线形情形时，可以简单地使用预测变量的二次多项式、三次多项式或更高阶多项式来**扩大特征空间**，因此优化问题将变为：
   $$
   \begin{align}
   &\max_{\beta_0,\beta_1,...,\beta_p,\epsilon_1,\epsilon_2,...,\epsilon_n}M\\
   s.t. \\&y_i(\beta_0+\sum^p_{j=1}\beta_{j1}x_{ij}+\sum^p_{j=1}\beta_{j2}x^2_{ji})\geq M(1-\epsilon_i)\\
   &\sum^n_{i=1}\epsilon_i\leq C,\epsilon_i\geq 0,\sum^p_{j=1}\sum^2_{k=1}\beta_{jk}^2=1
   \end{align}
   $$
   扩大特征空间的方法有很多，但是如果处理不当，将会得到数量庞大的特征。

2. **支持向量机（support vector machine，SVM）**是支持向量分类器在非线形情形下的一个拓展，使用了**核函数（kernel）**的方式来扩大特征空间，即将支持向量分类器中$f(\mathbf{x})$的内积改为$K(\mathbf{x}_i,\mathbf{x}_{i'})$，形式为：
   $$
   f(\mathbf{x})=\beta_0 + \sum_{i\in S}\alpha_iK(\mathbf{x},\mathbf{x}_i)
   $$
   核函数是一类用来衡量观测之间的相似性的函数，线性核函数等价于支持向量分类器中的内积形式，是基于皮尔逊相关系数来衡量的。也可以选择其他形式的核函数，例如：

   * $K(\mathbf{x}_i,\mathbf{x}_{i'})=(1+\sum^p_{j=1}x_{ij}x_{i'j})^d$，该式为自由度为$d$的**多项式核函数（polynomial kernel）**，从本质上来说，它是在与多项式的自由度$d$有关的高维空间中拟合支持向量分类器，而不是在原始的特征空间中拟合支持向量分类器。
   * $K(\mathbf{x}_i,\mathbf{x}_{i'})=\exp(-\gamma\sum^p_{j=1}(x_{ij}-x_{i'j})^2)$，该式为最常用且最受欢迎的**径向基核函数（radial basis function kernel，RBF）**或**高斯核函数（Gaussian kernel）**。以欧式距离衡量，如果测试观测离训练观测很远，RBF值将很小，反之，该值将很大，因此RBF核是一种**局部方法**，**只有测试观测周围的训练观测对测试观测的预测类别有影响**。

   基于以上两种核函数的支持向量机的分类结果如下图所示。

   <img src="images\ch9\9-5.png" style="zoom:50%;" />

> 使用核函数而不使用高次多项式扩大特征空间，在于核函数具有明确的计算量，即为$\binom{n}{2}$，而后者在扩大的特征空间中，没有明确的计算量。

### 9.1.4 SVM的多分类

&emsp;&emsp;虽然SVM是一个二分类的算法，但是也可以拓展到多分类的任务重，常用的方法为：

* **1对1（one versus one），或称所有成对（all pairs）**。设响应变量的类别数$K>2$，一类对一类分类法需要构建**$\binom{K}{2}$个SVM**模型，每个SVM用来分隔两个类别。使用所有$\binom{K}{2}$个SVM对一个测试观测进行分类，**测试观测的最终类别即为预测次数最多的那一类**。
* **1对多（one versus all）**。设响应变量的类别数$K>2$，用**$K$个SVM**来拟合数据，每个SVM对$K$个类别中的1个类别的观测和其他$K-1$个类别观测进行比较，把**观测预测为使得$\beta_{0k}+\beta_{1k}x^*_1+\beta_{2k}x^*_2+...+\beta_{pk}x^*_p$最大化的那个类别**。

## 9.2 题目答案

均为画图，略

# 第十章 无监督学习

&emsp;&emsp;**无监督学习（unsupervised learning）**的主要兴趣并非预测（因为没有响应变量$Y$），而在于由$X_1,X_2,...,X_p$构成的观测空间中一些有价值的模式，如预测变量的线性组合（降维）或观测的集聚分布（聚类）。无监督学习通常也可视作**探索性数据分析（exploratory data analysis，EDA）**的一部分。

## 10.1 相关概念

### 10.1.1 主成分分析

1. **主成分分析（principal component analysis）**在第六章已经加以简单介绍，本章将介绍该方法的细节。主成分可以用少数几个综合原始数据集中大部分变异信息的**典型变量**概括数据集，其中这些典型变量即为各个主成分，是**预测变量的线性变换**。主成分方向被解释为特征空间中原始数据**高度变异**的方向，除了为模型数据降维外，PCA还可以作为数据的可视化工具。

2. PCA的思想是$n$个观测都存在$p$维空间中，但并不是所有的预测变量都有同等价值。所以PCA致力找那些有意义的变量，保证数据的离散程度（方差）最大，以**第一主成分**为例，其是变量标准化线性组合中**方差最大**的组合，如下所示：
   $$
   Z_1=\phi_{11}X_1+\phi_{21}X_2+...+\phi_{p1}X_p
   $$
   其中，$\phi_{j1}$是第一主成分的**载荷（loading）**，进一步的，第一主成分的载荷变量为$\boldsymbol{\phi}_1=(\phi_{11}\quad\phi_{21}\quad...\quad\phi_{p1})^T$。计算第一主成分需要保证在$\sum_{j=1}^p \phi^2_{j1}=1$的条件下方差最大，假定$\mathbf{X}$中的每个变量都经过中心化处理，其均值为0（列方向上均值为0），优化问题为：
   $$
   \max_{\phi_{11},...,\phi_{p1}}\frac{1}{n}\sum^n_{i=1}(\sum_{j=1}^p\phi_{j1}x_{ij})^2\\
   s.t.\  \sum_{j=1}^p\phi_{j1}^2=1
   $$
   其中，$\sum_{j=1}^p\phi_{j1}x_{ij}$可记为$z_{i1}$，即为第一主成分的得分，可认为是将**数据点在该主成分载荷向量上的投影**。求解该优化问题涉及到线性代数，详细推导不在此笔记中开展，在后续算法专题笔记中将详细介绍。

3. 第二主成分$Z_2$与$Z_1$正交，即$Z_2$是与$Z_1$不相关的各种线性组合中方差最大的一个。其载荷向量为$\boldsymbol{\phi}_2=(\phi_{12}\quad\phi_{22}\quad...\quad\phi_{p2})^T$，计算载荷向量需在$Z_1$相似的优化问题基础上，增加令$\boldsymbol{\phi}_1$和$\boldsymbol{\phi}_1$垂直的限定条件。从另一个角度，**主成分方向$\boldsymbol{\phi}_1,\boldsymbol{\phi}_2,\boldsymbol{\phi}_3,...$是矩阵$\mathbf{X}^T\mathbf{X}$的特征向量依次排序**，主成分的方差是其**特征根**，**数据最多可以有$min(n-1,p)$个主成分**。

4. 主成分的另一种解释：主成分提供了一个**与观测最为接近的低微线性空间**。以第一主成分为例，它是$p$维空间中一条最接近$n$个观测的线（用平均平方欧式距离度量接近程度）。在该解释下，**前$M$个主成分得分向量和前$M$个主成分载荷向量为第$i$个观测$x_{ij}$提供了在$M$维欧式距离意义下最优的近似**（设原始数据矩阵$\mathbf{X}$经过列中心化），形式如下：
   $$
   x_{ij}\approx\sum^M_{m=1}z_{im}\phi_{jm}
   $$
   其中，$z_{im}$为第$m$个主成分第$i$个观测的得分，$\phi_{jm}$为第$m$个主成分第$j$个载荷。当$M$足够大时，可以给数据一个近似，特别的，当$M=\min(n-1,p)$时，上式等号成立。

5. 进行PCA时需要注意一下方面：

   * 在PCA之前，**变量应该被标准化处理**，即列的均值为0，方差为1。PCA结果受变量测量尺度的影响，是否进行标准化处理的对比如下图所示，但如果各特征的测量单位相同，可以选择不标准化。

     <img src="images\ch10\10-1.png" style="zoom:50%;" />

   * 在不考虑符号变化的情况下，**每个主成分载荷向量都是唯一的**。**符号可能不同**是因为每个主成分载荷向量在$p$维空间中有一个特定的方向，转换符号不应该有任何影响，因为其方向不变。

   * 主成分分析时，有一个自然的问题：每个主成分能代表多少的信息，或者解释了多少数据的变化？此时**方差解释比率（proportion of variance explained，PVE）**可以提供该问题的参考。假设数据集已经中心化处理，总方差定义为：
     $$
     \sum^p_{j=1}Var(X_j)=\sum^p_{j=1}\frac{1}{n}\sum^n_{i=1}x^2_{ij}
     $$
     第$m$个主成分的方差为：
     $$
     \frac{1}{n}\sum^n_{i=1}z^2_{im}=\frac{1}{n}\sum^n_{i=1}(\sum^p_{j=1}\phi_{jm}x_{ij})^2
     $$
     因此，第$m$各主成分的PVE为
     $$
     PVE_m = \frac{\sum^n_{i=1}(\sum^p_{j=1}\phi_{jm}x_{ij})^2}{\sum^p_{j=1}\sum^n_{i=1}x^2_{ij}}
     $$
     为计算前$M$个主成分的累积PVE，可以简单地对前$M$个PVE进行求和，一共有$\min(n-1,p)$个主成分，其和为1.

   * 在无监督的任务中，决定主成分的数量不易，尝试用**肘部法**确定，即分析在某点处下一个主成分解释的方差比例是否突然减小，如下图所示，第二个主成分出现第1个肘。

     <img src="images\ch10\10-2.png" style="zoom:50%;" />

### 10.1.2 聚类分析

1. **聚类分析（clustring）**是在数据集中寻找子群或类的技术，期望**每个类内的观测彼此相似**，而**不同类中的观测彼此相异**，所以必须对2个或更多的观测进行建模。聚类算法种类繁多，本节将重点介绍最知名的两种聚类方法：$K$均值聚类和系统聚类。

2. **$K$均值聚类（$K$-means clustring）**是一种将数据集分为$K$个不重复类的简单快捷的方法，**需要预先设定聚类数目参数$K$**，用$C_1,...,C_K$表示在每个类中包含观测指标的集合，其集合满足一下两个性质：

   * $C_1 \bigcup C_2 \bigcup ... \bigcup C_k = {1, ..., n}$，即每个观测属于$K$个类中的至少一个类。
   * $C_k \bigcap C_k'=\phi$对每个$k\neq k'$都成立，即类与类之间没有重叠。

   $K$均值聚类发算法的思想在于使**类内差异尽可能小**，第$C_k$类的类内差异是对第$C_k$类中观测互不相同程度的度量$W(C_k)$，以最常使用的欧式距离为例，$W(C_k)=\frac{1}{|C_k|}\sum_{i,i'\in C_k}\sum^p_{j=1}(x_{ij}-x_{i'j})^2$，需要解决如下最小化问题：
   $$
   \min_{C_1,...,C_k}\ \sum^K_{k=1}\frac{1}{|C_k|}\sum_{i,i'\in C_k}\sum^p_{j=1}(x_{ij}-x_{i'j})^2
   $$
   得到这样该优化问题的解很困难，需要尝试$K^n$种分配，因此$K$均值算法给了一种**局部最优**的解决方案：

   >**$K$均值聚类法**
   >
   >* 为每个观测随机分配一个从1到$K$的数字。这些数字可以看作对这些观测的初始类。（相当于随机选择$K$个簇中心）
   >* 重复下列操作，直到类的分配停止为止：
   >  * 分别计算$K$个类的类中心，其中类中心是类中$p$维观测向量的均值向量。
   >  * 将每个观测分配到距离其最近的类中心所在的类中。

   以下基于欧式距离的最优化恒等式有助于理解算法步骤：
   $$
   \frac{1}{|C|}\sum_{i,i'\in C_k}\sum^p_{j=1}(x_{ij}-x_{i'j})^2=2\sum_{i\in C_k}\sum^p_{j=1}(x_{ij}-\bar{x}_{kj})^2
   $$
   其中$\bar{x}_{kj}=\frac{1}{|C_k|}\sum_{i\in C_k}x_{ij}$是第$C_k$类中第$j$个分量的均值。当上述算法的结果不再改变时，分类就达到了局部最优，分配的过程如下图所示。

   <img src="images\ch10\10-3.png" style="zoom:50%;" />

   > 注：算法停止时的聚类未必是总体最优解，由于第一步的随机性，所以需要运行$K$均值算法，然后从中选择一个最优的方案。

3. **层次聚类法（hierarchical clustring）**不需要预先规定聚类数目，且可以输出各个观测的树型表示，称为**谱系图（dendrogram）**。本笔记将介绍**自下而上的（bottom-up）**也称为**凝聚法（agglomerative）**的层次聚类。其分类步骤如下：

   >**层次聚类法**
   >
   >* 计算$n$个观测中所有$\binom{n}{2}=n(n-1)/2$对每两个数据之间的相异度，将每个观测分为一类。
   >* 令$i=n,n-1,...,2$：
   >  * 在$i$个类中，比较任意两个类间的相异度，找到最相似的那一对，并将其结合。**用两个类之间的相似度表示两个类在谱系图中交汇的高度。**
   >  * 计算剩下的$i-1$个新类中，每两个类间的相异度。

   其中，两个类间相似度的比较方法有4种常见形式：

   * **最长距离法（complete）**，计算A类和B类之间的所有观测的相异度，并记录最大的相异度。
   * **最短距离法（single）**，计算A类和B类之间的所有观测的相异度，并记录最小的相异度。该方法会导致一个接一个的汇合延伸拖尾的类。
   * **类平均法（average）**，计算A类和B类之间的所有观测的相异度，并记录这些相异度的平均值。
   * **重心法（centroid）**，计算A类中心（长度为$p$的均值向量）和B类中心的相异度。该方法会导致一种不良的倒置现象发生。

   基于最长距离法根据观测得到谱系图，如下图所示。其中，越早（在树的较低处）汇合的各组观测之间越相似，越晚（接近树顶）汇合的各组观测之间的差异会越大。**纵轴上的枝条初次汇合高度表示两个观测的差异程度**。谱系图中有另外$2^{n-1}$种可能的排序，其中$n$是叶子的数量。谱系图上**切割高度控制了类的数量**。

   <img src="images\ch10\10-4.png" style="zoom:50%;" />

4. 在聚类前，首先要根据实际问题，选择恰当的相似度指标，有些情况度量观测向量的**距离**较好（可以选择欧式距离），有些情况度量观测的**相似度**较好（可以选择皮尔逊相关系数）。此外，根据变量的实际情况，如度量单位或出现的频次，需要考虑**标准化**处理，否则对聚类结果有较大影响。

## 10.2 题目答案

1. 证明$K$均值聚类算法中的恒等式，并解释为何最小化式的在每次迭代后减小。
   $$
   \begin{align}
   &证明：\frac{1}{|C_k|} \sum\limits_{i,i^{\prime} \in C_k} \sum\limits_{j=1}^p (x_{ij} - x_{i^\prime j})^2 = 
   2 \sum\limits_{i \in C_k} \sum\limits_{j=1}^{p} (x_{ij} - \bar{x}_{kj})^2
   \\
   左式&=\frac{1}{|C_k|} \sum\limits_{i,i^{\prime} \in C_k} \sum\limits_{j=1}^p ((x_{ij} - \bar{x}_{kj}) - (x_{i^\prime j} - \bar{x}_{kj}))^2
   \\
   &= \frac{1}{|C_k|} \sum\limits_{i,i^{\prime} \in C_k} \sum\limits_{j=1}^p ((x_{ij} - \bar{x}_{kj})^2 - 2 (x_{ij} - \bar{x}_{kj})(x_{i^\prime j} - \bar{x}_{kj}) + (x_{i^\prime j} - \bar{x}_{kj})^2)
   \\
   &= \frac{|C_k|}{|C_k|} \sum\limits_{i \in C_k} \sum\limits_{j=1}^p (x_{ij} - \bar{x}_{kj})^2 +
     \frac{|C_k|}{|C_k|} \sum\limits_{i^{\prime} \in C_k} \sum\limits_{j=1}^p (x_{i^\prime j} - \bar{x}_{kj})^2 -
     \frac{2}{|C_k|} \sum\limits_{i,i^{\prime} \in C_k} \sum\limits_{j=1}^p (x_{ij} - \bar{x}_{kj})(x_{i^\prime j} - \bar{x}_{kj})
   \\
   \because &\frac{2}{|C_k|} \sum\limits_{i,i^{\prime} \in C_k} \sum\limits_{j=1}^p (x_{ij} - \bar{x}_{kj})(x_{i^\prime j} - \bar{x}_{kj})=
   \frac{2}{|C_k|} \sum\limits_{i \in C_k} \sum\limits_{j=1}^p (x_{ij} - \bar{x}_{kj})\sum\limits_{i^{\prime} \in C_k}(x_{i^\prime j} - \bar{x}_{kj})=0\\
   \therefore原式&= 2 \sum\limits_{i \in C_k} \sum\limits_{j=1}^p (x_{ij} - \bar{x}_{kj})^2 + 0
   \end{align}
   $$
   结合上式等价条件可以看出，当类均值与类观测的距离一直减少时，最小化式的值也将减少，即类内的观测越相似。

其余略。

# 读后感

&emsp;&emsp;本书的学习已经结束，读了两遍后，更加觉得这是一本不可多得好书。在介绍机器学习的全局概念之时，也会娓娓道来各模型算法的细节及其统计含义，并配合了R应用（虽然对R不熟），对整个知识体系的建立大有裨益。唯一不足的是本书版本较老，一些更加前沿的模型算法未有涉及，不过作为机器学习的入门以及ESL的导读书籍，ISLR已经发挥了非常大的作用。

&emsp;&emsp;在后续的学习中，将针对机器学习以及深度学习模型的细节以及应用（底层、调包，均基于Python）进行开展，并以算法或知识点的专题呈现，这是一个长期的过程，后续将不定期更新相关内容。