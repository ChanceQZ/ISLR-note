# 概率论与数理统计笔记

# 第一章 导论

&emsp;&emsp;**统计学习（statistical learning）**是一套以理解数据为目的庞大工具集，可分为**监督式（supervised）**学习和**非监督式（unsupervised）**学习。

# 第二章 统计学习

## 2.1 相关概念

1. 统计学习是关于估计$f(\cdot)$的一系列方法，其中$f(\cdot)$为一个定量的响应变量$Y$和$p$个不同的预测变量$X=(X_1,X_2,...,X_p)$之间的关系，一般形式如下：
   $$
   Y=f(X)+\epsilon
   $$
   其中，$\epsilon$是随机误差项（error term），与$X$独立，且均值为0.

   误差项包含了一下因素：
   
   * 真实的关系可能不是$f(\cdot)$，例如在简单线性回归估计中，实际关系可能并不是线性的；
   * 可能是其他变量导致了$Y$的变化；
   * 可能存在测量误差。
   
2. 估计$f(\cdot)$的<u>主要原因</u>可分为**预测（prediction）**和**推断（inference）**，其中：

   * **预测**

     关注预测的结果，不关注模型的可解释性和变量之间的关系，可表示为：
     $$
     \hat{Y}=\hat{f}(X)
     $$
     其中$\hat{Y}$表示$Y$的预测值，依赖于两个量，**可约误差（reducible error）**和**不可约误差（irreducible error）**，可约误差可通过改进统计学习方法降低，而不可约误差$\epsilon$是无法降低的，所以即使得到一个$f$的精确估计，预测仍然存在误差，预测的均方误差可表示为
     $$
     \begin{align}
     E(Y-\hat{Y})^2=&E[f(X)+\epsilon-\hat{f}(X)]^2\\
     =&\underbrace{[f(X)-\hat{f}(X)]^2}_{可约误差}+\underbrace{Var(\epsilon)}_{不可约误差}
     \end{align}
     $$
     
* **推断**
  
  目标不是为了预测$Y$，而是想明白$X$和$Y$之间的关系，可以描述为以下问题：
  
  * 哪些预测变量与响应变量相关？
     * 响应变量与每个预测因子之间的关系是什么？
     * $Y$与每个预测变量的关系是否能用一个线性方程概括，还是需要更复杂的形式？
  
3. 估计$f(\cdot)$的<u>方法</u>可分为**参数方法**和**非参数方法**：

   * **参数方法**

     参数方法指有一定的形式或形状的模型，如假设$f(\cdot)$是线性的，则具有如下形式：
     $$
     f(X)=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p
     $$
     在模型选完后则需要使用训练数据去**拟合（fit）**或**训练（train）**模型，即估计参数$\beta_0,\beta_1,...,\beta_p$。参数方法**最大的优势**就是可以将$f(\cdot)$假设为具体的参数形式可简化估计。然而**缺陷**则是选定的模型并非与真正 的$f(\cdot)$在形式上是一致的。**非参数方法适合推断的问题。**

   * **非参数方法**

     非参数方法不需要对函数$f$的形式事先做明确的假设。**优势**是不限定函数$f(\cdot)$的具体形式，可能在更大的范围选择更适宜$f(\cdot)$，然而有**最致命的缺陷**即无法将估计$f(\cdot)$的问题简化成对参数的估计，需要大量的数据（远远超出参数方法所需要的）。

4. **监督学习**和**非监督学习**的区别在于**前者有响应变量（标签）**，形如$\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$而**后者无响应变量（标签）**，形如$\{x_1,x_2,...,x_n\}$。
5. 根据变量的**定量（连续）**和**定性（离散）**类型，可将任务分为**回归**和**分类**问题，前者如对GDP、PM2.5的预测，后者如对动物、生病与否的识别。

## 2.2题目答案

1. (a) 当**样本量n非常大**，**预测变量数p很小**时，这样容易欠拟合，所以一个**光滑度更高的学习模型更好**。

   (b) 当**样本量n非常小**，**预测变量数p很大**时，这样容易过拟合，所以一个**光滑度更小的学习模型更好**。

   (c) 当预测变量与响应变量之间的关系是**非线性**时，说明光滑度小的模型会容易欠拟合，所以**光滑度高的模型更适合**。

   (d) 当**误差项的方差$\sigma^2=Var(\epsilon)$极大**时，因为方差是指用一个不同的训练数据集估计f时，估计函数的改变量。一般来说，**光滑度越高的统计模型有更高的方差**，所以这里适合**光滑度小的模型**。

2. (a) 收集了美国500强公司的数据。每个公司都记录了利润、员工人数、产业类型和CEO的工资。（回归，推断）

   (b) 考虑研发一个新产品，希望知道它会成功还是失败，收集了先前研发的20个相近产品的数据，并记录它们成功或失败的状态，以及其他若干变量。（分类，预测）

   (c) 兴趣在于预测美元的百分比变化率随全球股市周变动的变化规律，为此收集了2012年所有的周数据。（回归，预测）

3. **偏差：**度量了学习算法的期望预测与真实结果偏离程度，即刻画了学习算法本身的拟合能力。

   **方差：**度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响，或者说学习算法的稳定性。

   **训练误差：**模型在训练集上的误差。

   **测试误差：**测试集上的误差。

   **贝叶斯（或不可约）误差：**贝叶斯误差也叫最优误差，通俗来讲，它指的就是现有技术下人和机器能做到最好的情况下，出现的误差。比如图像识别和语音识别这类处理自然数据的任务，人类水平和贝叶斯水平相差不远，通常用人类水平来近似成贝叶斯水平，也就是说人的误差可以近似地看成贝叶斯误差。

   **<u>偏差 = 贝叶斯误差 + 可避免偏差</u>**

   <img src="images\ch2\3(a).jpg" style="zoom:50%;" />

4. 略

5. 一个光滑度高的回归模型或者分类模型，能够更好的拟合非线性模型，偏差更小。但是模型越光滑，所需要计算的参数就越多，而且容易过拟合，方差更大。当我们更想预测，而不是推断的时候，我们优先考虑光滑度高的模型。光滑度低的模型相反。

6. (a) 参数方法是一种基于模型估计的两阶段方法。优点：它把估计$f(\cdot)$的问题简化到估计一组参数，对f假设一个具体的参数形式将简化对$f(\cdot)$的估计，因为估计参数是更为容易的，不需要拟合任意一个函数$f(\cdot)$。缺点：选定的模型并非与实际的f形式上一致，而且还有过拟合的可能情况。

   (b) 非参数方法不需要对函数f的形式实现做明确说明的假设。相反，这类方法追求的接近数据点的估计，估计函数在去粗和光滑处理后尽量可能与更多的数据点接近。优点：不限定函数$f(\cdot)$的具体形式，可以更大的范围选择更适宜的$f(\cdot)$形状的估计。缺点：无法将估计$f(\cdot)$的问题简单到对少数参数进行估计的问题，所以往往需要大量的观察点。

# 第三章 线性回归

## 3.1 相关概念

### 3.1.1 简单线性回归

1. **简单线性回归（Simple linear regression）**假定$X$和$Y$之间存在线性关系，其形式为：
   $$
   Y\approx\beta_0+\beta_1X
   $$
   表示**$Y$对$X$的回归**，其中$\beta_0$和$\beta_1$分别表示为模型的截距和斜率，被称为模型的**系数（coefficient）**或**参数（parameter）**。在给定数据时，也可表示为：
   $$
   \hat{y}=\hat{\beta}_0+\hat{\beta}_1x
   $$
   其中，$\hat{y}$表示$X=x$的基础上对$Y$的预测。

2. 评价模型拟合效果可通过测量**接近程度（closeness）**，常用观测的相应值和预测的相应值之间的差距作为参考，定义**残差平方和（Residual sum of square，RSS）**为：
   $$
   \begin{align}
   RSS=&e_1^2+e_2^2+...+e_n^2\\
   =&(y_1-\hat{\beta_0}-\hat{\beta}_1x_1)^2+(y_2-\hat{\beta_0}-\hat{\beta}_1x_2)^2+...+(y_n-\hat{\beta_0}-\hat{\beta}_1x_n)^2\\
   =&\sum_{i=1}^n(y_i-\hat{y}_i)^2
   \end{align}
   $$

3. **最小二乘估计（Least squares coefficient estimate）**期望将模型的RSS达到最小，如图所示，其中每条线段代表一个残差。

   <img src="images\ch3\3-1.png" style="zoom:50%;" />

   可通过微积分运算，使简单线性回归的RSS达到最小的参数估计为：
   $$
   \hat{\beta}_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\\
   \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}
   $$
   其中，$\bar{y}\equiv\frac{1}{n}\sum_{i=1}^ny_i$和$\bar{x}\equiv\frac{1}{n}\sum_{i=1}^nx_i$是样本均值。

   >**最小二乘估计的推导**
   >$$
   >\begin{align}
   >&RSS=\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta}_1x_i)^2 \\
   >\\
   >\overset{对参数求偏导}{\Rightarrow}
   >&\left\{\begin{aligned}
   >\frac{\partial{RSS}}{\partial{\hat{\beta}_0}}=&2\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta}_1x_i)\\ 
   >\frac{\partial{RSS}}{\partial{\hat{\beta}_1}}=&2\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta}_1x_i)(-x_i) \\
   >\end{aligned}\right.\\
   >\\
   >\overset{令其为0}{\Rightarrow}
   >&\left\{\begin{aligned}
   >&\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta}_1x_i)&=0\\ 
   >&\sum_{i=1}^n(y_i-\hat{\beta_0}-\hat{\beta}_1x_i)(x_i)&=0 \\
   >\end{aligned}\right.\\
   >\\
   >\overset{化简得}{\Rightarrow}
   >&\left\{\begin{aligned}
   >&n\bar{y}-n\hat{\beta}_0-n\hat{\beta}_1\bar{x}&=0 \\ 
   >&\sum_{i=1}^nx_iy_i-n\hat{\beta}_1\bar{x}-\hat{\beta}_1\sum_{i=1}^nx_i^2&=0 \\
   >\end{aligned}\right.\\
   >\\
   >\overset{化简得}{\Rightarrow}
   >&\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}\\
   >&\hat{\beta}_1=\frac{\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}^nx_i^2-n\bar{x}^2}\\
   >&\quad=\frac{\sum_{i=1}^nx_iy_i-\sum_{i=1}^n\bar{x}\bar{y}}{\sum_{i=1}^nx_i^2-\sum_{i=1}^n\bar{x}^2}\\
   >&\quad=\frac{\sum_{i=1}^n(x_iy_i-yi\bar{x}-x_i\bar{y}+\bar{x}\bar{y})}{\sum_{i=1}^n(x_i^2-2x_i\bar{x}+\bar{x}^2)}\\
   >&\quad=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
   >\end{align}
   >$$

4. 线性回归是**无偏估计**，同时也遵从估计的**相合性**（遵循格里纹科定理）原则，即如果在特定数据集的基础上估计$\beta_0$和$\beta_1$，则估计值不会恰等于$\beta_0$和$\beta_1$，但是，如果对从大量数据集上得到的估计值求平均，他们的均值恰为真值。

5. 一般的估计问题中，可以使用**标准误差（Standard error， 写作$SE(\hat{\mu})$）**评价估计的准确性，表示估计$\hat{\mu}$偏离$\mu$的实际值的平均量，形式为：
   $$
   Var（\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}
   $$
   其中，$\sigma$是变量$Y$的每个现实值$y_i$的标准差。同理，也可以探究$\hat{\beta}_0$和$\hat{\beta}_1$与真实值$\beta_0$和$\beta_1$的接近程度，形如：
   $$
   SE(\hat{\beta}_0)^2=\sigma^2[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n(x_i-\bar{x})^2}]\\
   SE(\hat{\beta}_1)^2=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}
   $$

   > **系数标准误差的推导：**
   >
   > 暂略，后续补

6. 标准误差可用于计算**置信区间（Confidence interval）**，对于线性回归模型，$\beta_1$和$\beta_0$的95%置信区间约为$\hat{\beta}_1\pm2\cdot SE(\hat{\beta}_1)$和$\hat{\beta}_0\pm2\cdot SE(\hat{\beta}_0)$.

7. 标准误差可用对系数进行**假设检验**，其中最常用的检验包括零假设（$X$和$Y$之间没有关系）和备择假设（$X$和$Y$之间有一定的关系），使用$t$统计量测量$\hat{\beta}_1$偏离0的标准偏差，其形式为：
   $$
   t=\frac{\hat{\beta}_1-0}{SE(\hat{\beta}_1)}
   $$
   对于零假设，即假设$\beta_1=0$，计算任意观测值大于等于$|t|$的概率即可，该概率为$p$**值**，可以解释为：一个很小的p值表示，在预测变量和相应变量之间的真实关系未知的情况下，不太可能完全由于偶然而观察到预测变量和相应变量之间的强相关。因此，**如果$p$值很小，可以推断预测变量和相应变量之间存在关联，即可拒绝零假设**，典型的拒绝零假设的临界$p$值是5%或1%.

8. 评价模型的准确性有两个指标，一是**残差标准误（Residual standard error，$RSE$）**，是对模型中$\epsilon$的标准偏差的估计，形式为：
   $$
   RSE=\sqrt{\frac{1}{n-1}RSS}=\sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat{y})^2}
   $$
   RSE被认为是对模型**失拟（lack of fit）**的度量，$\hat{y}_i$与$y_i$相差很大，那么RSE可能是相当大的，这表明该模型未能很好地拟合数据。

9. 另一是**$R^2$统计量**，相比较于RSE对数据失拟的绝对测度方法，$R^2$统计量采取比例（被解释方差的比例）形式，其形式为：
   $$
   R^2=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}
   $$
   其中，$TSS$是总平方和，测量了相应变量$Y$的总方差，可认为是在执行回归分析之前相应变量中的固有变异性，$RSS$测量的是进行回归后仍无法解释的变异性，因此$TSS-RSS$测量的是相应变量进行回归之后被解释（或被消除）的变异性，则**$R^2$测量的是$Y$变异中能被$X$解释的部分所占比例**。

   >**注：**在简单线性回归中，$R^2$统计量等价于$X$和$Y$的相关系数，即$R^2=r^2$，证明如下：
   >$$
   >\begin{align}
   >Cor(X, Y)&=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}\\
   >\\
   >R^2&=\frac{\sum_{i=1}^n(y_i-\bar{y})^2-\sum_{i=1}^n(y_i-\hat{y})^2}{\sum_{i=1}^n(y_i-\bar{y})^2}\\
   >\\
   >令A&=\sum_{i=1}^n(y_i-\bar{y})^2-\sum_{i=1}^n(y_i-\hat{y})^2\\
   >&=\sum_{i=1}^n[(y_i-\bar{y})^2-(y_i-\hat{y})^2]\\
   >&=\sum_{i=1}^n(y_i-\bar{y}-y_i+\hat{y}_i)(y_i-\bar{y}+y_i-\hat{y}_i)\\
   >&=\sum_{i=1}^n(\hat{y}_i-\bar{y})(2y_i-\bar{y}-\hat{y}_i)\\
   >&=\sum_{i=1}^n(\hat{\beta}_0+\hat{\beta}_1x_i-\bar{y})(2y_i-\bar{y}-\hat{\beta}_0-\hat{\beta}_1x_i)\\
   >&=\sum_{i=1}^n\hat{\beta}_1(x_i-\bar{x})[2y_i-2\bar{y}-\hat{\beta}_1(x_i-\bar{x})]\\
   >&=\hat{\beta}_1[2\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})-\hat{\beta}_1\sum_{i=1}^n(x_i-\bar{x})^2]\\
   >&=\hat{\beta}_1\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})\\
   >&=\frac{[\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})]^2}{\sum_{i=1}^n(x_i-\bar{x})^2}\\
   >\\
   >\therefore R^2&=\frac{A}{\sum_{i=1}^n(y_i-\bar{y})^2}=\frac{[\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})]^2}{\sum_{i=1}^n(x_i-\bar{x})^2\sum_{i=1}^n(y_i-\bar{y})^2}=Cor^2
   >\end{align}
   >$$
   >

### 3.1.2 多元线性回归

1. 多元线性回归涉及了$p$个不同的预测变量，该模型形式为：
   $$
   Y=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p+\epsilon
   $$
   其中，$X_j$代表第$j$个预测变量，$\beta_j$代表第$j$预测变量和相应变量之间的关联。在给定数据时，其参数估计形式为：
   $$
   \hat{y}=\hat{\beta}_0+\hat{\beta}_1x_1+\hat{\beta}_2x_2+...+\hat{\beta}_px_p
   $$

2. 多元线性回归的最小二乘估计原理同简单线性回归一样，期望将模型的RSS达到最小，如图所示
   

<img src="images\ch3\3-4.png" style="zoom:50%;" />

   但是需要用矩阵代数形式表示：
$$
\pmb{\hat{\beta}}=(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\pmb{y}
$$
   其中，$\pmb{\hat{\beta}}$为参数向量，$\pmb{X}$为预测变量矩阵，$\pmb{y}$为响应变量向量。

   >**最小二乘估计的推导**
   >$$
   >\begin{align}
   >\pmb{y}&=\pmb{X}\pmb{\beta}\\
   >RSS&=(\pmb{y}-\pmb{X}\hat{\pmb{\beta}})^T(\pmb{y}-\pmb{X}\hat{\pmb{\beta}})\\
   >&=\pmb{y}^T\pmb{y}-\pmb{y}^T\pmb{X}\pmb{\hat{\beta}}-\pmb{\hat{\beta}}^T\pmb{W}^T\pmb{y}+\pmb{\hat{\beta}}^T\pmb{X}^T\pmb{X}\pmb{\hat{\beta}}\\
   >\\
   >由\frac{\part{\pmb{a}^T\pmb{x}}}{\part{\pmb{x}}}&=\frac{\part{\pmb{x}^T\pmb{a}}}{\part{\pmb{x}}}=\pmb{a},\quad \frac{\part{\pmb{x}^T\pmb{A}\pmb{x}}}{\part{\pmb{x}}}=(\pmb{A}+\pmb{A}^T)\pmb{x}\quad得\\
   >\\
   >\frac{\part{RSS}}{\part{\pmb{\hat{\beta}}}}&=0-\pmb{X}^T\pmb{y}-\pmb{X}^T\pmb{y}+(\pmb{X}^T\pmb{X}+\pmb{X}^T\pmb{X})\pmb{\hat{\beta}}\\
   >&=2\pmb{X}^T(\pmb{X}\pmb{\hat{\beta}}-\pmb{y})\\
   >\\
   >当满秩时，令\frac{\part{RSS}}{\part{\pmb{\hat{\beta}}}}&=0\\
   >\therefore \pmb{X}^T\pmb{X}\pmb{\hat{\beta}}-\pmb{X}^T\pmb{y}&=0\\
   >\pmb{\hat{\beta}}&=(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\pmb{y}
   >\end{align}
   >$$

3. 虽然响应变量和预测变量在简单线性回归中具有较高的$R^2$，但**若增加其他预测变量后，原始的预测变量可能将不具有统计意义（$p$值低）**，这是因为简单回归模型忽视了预测变量之间的相互关系，可能存在着内部联系。
4. 多元线性回归将关注一下几个重要问题：
   * 预测变量$X_1,X_2,...,X_p$中是否至少有一个可以用来预测响应变量？
   * 所有预测变量都有助于解释$Y$吗？或仅仅是预测变量的一个子集对预测有用？
   * 模型对数据的拟合程度如何？
   * 给定一组预测变量的值，响应值应预测为多少？所作预测的准确程度如何？

5. 在简单线性回归中，可以使用$p$值衡量模型的有效性，但$p$值是针对每一个预测变量的，且**在实际中，即使任何预测变量与响应变量都不相关，但仍有很小的几率使得部分$p$值小于0.05，因此单独使用$t$统计量和$p$值将很有可能错误地得出相关性的结论**。因此需要计算模型整体的评价指标，$F$统计量：
   $$
   F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}
   $$
   因为$E\{RSS/(n-p-1)\}=\sigma^2$，若零假设为真，$E\{TSS-RSS)/p\}=\sigma^2$，所以**当响应变量和预测变量无关时，$F$统计量应接近1**。一个较大的$F$统计量表示，至少有一个预测变量与响应变量相关，**若$n$很大，即使$F$统计量只是略大于1，可能也仍然提供了拒绝零假设的证据，相反，若$n$很小，则需要较大的$F$统计量才能拒绝零假设**。

6. 对于本章模型，若$p<n$，则可以使用相应的评价指标，如$F$统计量等，相反，不可以使用上述的指标，因为无法使用最小二乘法估计模型参数。

7. 在多元线性回归模型中，**最常见的情况是响应变量仅与预测变量的一个子集相关。**因此，需要对预测变量进行筛选，当预测变量很少时，可一个一个迭代筛选，但是数量较多时，则需要以下方法：

   * **向前选择**：从**零模型**（只包含截距）开始，对所有预测变量与响应变量建立简单线性回归模型，并将RSS最小的预测变量纳入零模型中，直到满足某种规则时停止。
   * **向后选择**：从包含所有变量的模型开始，依次删除$p$最低的变量，循环操作，知道满足某种规则时停止。
   * **混合选择**：先做向前选择，在做向后选择。

   > **注：**当$p>n$时，不能使用向后选择，而向前选择在各种情况下都适用。向前选择基于贪婪的模式，可将对模型没有“贡献”的变量纳入其中，可使用混合选择方法修正该问题。

8. 在简单回归中，$R^2$是响应变量和预测变量的相关系数的平方，在多元线性回归中，$R^2=Cor(Y,\hat{Y})^2$，即是响应值和线性模型拟合值的相关系数的平方。（其实本质上是一样的，在简单线性回归中，$\hat{Y}$只是$X$的线性变换，并不影响相关系数。）
9. 在多元线性回归中，预测变量间可能存在**协同效应（synergy）**或**交互作用（interaction）**，即组合使用这些预测变量比单独使用预测变量效果更好。

### 3.1.3注意事项

1. **定性预测变量**。大多数预测变量都是定量的（或者说是**连续型数据**），但有时预测变量会是定性的（或者说是**离散型数据**），如性别（男女）、种族（黄人、白人、黑人）等。以最简单的**二值预测变量**为例，可以将其创建**哑变量**（dummy variable），如基于性别变量创建新变量：
   $$
   x_i=\left\{\begin{aligned}
   1\quad 女性\\
   0\quad 男性
   \end{aligned}\right.\\
   $$
   回归模型可以表示为：
   $$
   y_i=\beta_0+\beta_1x_i+\epsilon_i=\left\{\begin{aligned}
   \beta_0+\beta_1+\epsilon_i \quad 女性\\
   \beta_0+\epsilon_i \quad 男性
   \end{aligned}\right.\\
   $$
   其中，$\beta_0$可以解释为男性的平均值，$\beta_0+\beta_1$为女性的平均值，$\beta_1$是男性和女性之间的差异值。

2. 标准的线性回归模型有两个重要的假设，即预测变量和相应变量是**可加**和**线性**的。前者假设预测变量之间是互相**独立**分布的，后者假设无论预测变量取何值，该预测变量引起相应变量的变化是**恒定**的。

   但是，在现实中并不满足这样的假设，比如预测变量之间**高度相**关，存在协同效应或交互作用，亦或预测变量和相应变量之间的真实关系并是**非线性**的，针对前者需要对变量进一步筛选或降维，后者则需要将模型假设修正为非线性。

3. 下面将介绍线性模型遇到的常见几个问题，分别是**数据本身存在非线性**、**误差项自相关**、**误差项方差非恒定**、**离群点**、**高杠杆点**和**共线性**。

   * **数据本身存在非线性**

     * 实际情况中，很少数据是满足线性的，可以根据**残差图**（Residual plot）识别非线性，如下图所示，左图的残差趋势为U形，表明真实的关系应该是非线性的，当将模型修正为非线性时，残差呈现随机分布，表明该修正提升了模型对数据的拟合度。

       <img src="images\ch3\3-9.png" style="zoom:50%;" />

     * **措施：**对预测变量使用费线性变换，如$logX, \sqrt{X}, X^2$，或者使用更先进的非线性方法。

   * **误差项自相关**
     * 如果误差项存在相关性，那么标准误的估计往往会低估了真实标准误。以时间序列或者是地理数据为例，这两类数据最为明显，相邻的观测呈现误差正相关的关系。
     * **措施：**差分法等。

   * **误差项方差非恒定**

     * 线性回归模型的另一个重要假设是误差项的方差是恒定的，$Var(\epsilon_i)=\sigma^2$.但通常情况下，误差项的方差并不恒定，可能随着相应值的增加而增加。如图所示，左图残差图呈漏斗形，表明误差方差非恒定。

       <img src="images\ch3\3-11.png" style="zoom:50%;" />

     * **措施：**使用凹函数对相应值做变换，如$logY$和$\sqrt{Y}$，结果如上图右边所示。

   * **离群点**

     * 离群点是指$y_i$远离模型预测值的点，可通过残差图识别离群点，但是难以使用定量化的方法描述离群点，为解决该问题，引入了**学生化残差**，其由残差$e_i$除以它的估计标准误得到。**学生化残差绝对值大于3的观测点可能是离群点**。
     * **措施：**直接剔除此观测点。**但是，一个离群点可能不是由失误导致的，而是暗示模型存在缺陷，比如缺少预测变量。**

   * **高杠杆点**

     * 高杠杆表示观测点$x_i$是异常的，如下图左边，观测点41具有高杠杆值，因为它的预测变量值比其他观测点都要大。**高杠杆点的观测往往对回归直线的估计有很大的影响（比离群点还大）**。

       <img src="images\ch3\3-13.png" style="zoom:50%;" />

       虽然在各预测变量的取值都在正常范围内，但从整体预测变量集的角度来看，它却是不寻常的，如上图中部所示。可以计算**杠杆统计量**：
       $$
       h_i=\frac{1}{n}+\frac{(x_i-\bar{x})^2}{\sum^n_{i'=1(x_{i'}-\bar{x})^2}}
       $$
       一个大的杠杆统计量对应一个高杠杆点，杠杆统计量$h_i$的取值总是在$\frac{1}{n}$和1之间，且所有观测的平均杠杆值总是等于$(p+1)/n$。

     * **措施：**剔除。

   * **共线性**

     * 共线性指两个或更多的预测变量高度相关。检测共线性的一个简单方法是看预测变量的相关系数矩阵，但当有多重共线性时（三个或更多变量间存在共线性），更优的检测方法是计算方差膨胀因子（Variance inflation factor，VIF）（不做介绍）。
     * **措施：**1.剔除；2.降维。

## 3.2 题目答案

1. 电视和广播的低p值表明对于电视和广播的零假设都是错误的。报纸的高p值表明对于报纸的零假设是正确的，即报纸该预测变量不具有统计意义。

2. 差距既是回归和分类任务之间的差距，前者适合连续变量的预测，后者适合离散变量的预测。

3. (a)和(b)略

   (c)错误，不能直接通过回归系数评判交互项的有效性，需要通过该交互项的p值。

4. 一组数据包括单个预测变量和定量响应变量（观测数=100），分别使用线性回归模型和三次项回归模型进行拟合

   (a)假设X和Y满足线性关系，三次项回归模型的训练RSS小于线性回归模型的训练RSS（因为高次模型的误差项更高）；

   (b)条件同(a)，三次项回归模型的测试RSS大于线性回归模型的测试RSS（因为真实的关系为线性）；

   (c)假设X和Y满足非线性关系且具体关系未知，三次项回归模型的训练RSS小于线性回归模型的训练RSS（因为高次模型的误差项更高）；

   (d)条件同(c)，则不能判断谁的测试RSS低，因为并不知道真实的关系离线性近还是离三次近。

5. 设$\hat{y}_i=x_i\hat{\beta}$，其中$\hat{\beta}=(\sum^n_{i=1}x_iy_i)/(\sum^n_{i'=1}x^2_{i'})$，证明：$\hat{y}_i=\sum^n_{i'=1}a_{i'}y_{i'}$

   证：
   $$
   \begin{align}
   \hat{y}_i=&x_i\hat{\beta}\\
   =&x_i\frac{\sum^n_{k=1}x_ky_k}{\sum^n_{z'=1}x^2_{z'}}\\
   =&\sum^n_{k=1}\frac{x_ix_k}{\sum^n_{z'=1}x^2_{z'}}y_k\\
   =&\sum^n_{i'=1}a_{i'}y_{i'}
   \end{align}
   $$

6. 在简单线性回归中，最小二乘线通过点$(\bar{x},\bar{y})$.

7. 证明简单线性回归中的$R^2$统计量等于X和Y之间的相关系数的平方。

# 第四章 分类

## 4.1 相关概念

### 4.1.1 分类问题概述

1. 分类问题是针对定性变量的，大部分基于不同类别的概率，将分类问题作为**概率估计**的一个结果。
2. 当类别数较多（大于2类），则线性回归不具有意义，因为类别数无法被定量表达，例如：1代表红色，2代表绿色，3代表蓝色，则使这些类型具有可度量性，与实际不符，但对于2分类（0-1分类）来说，线性回归具有一定的意义，但预测结果很容易超过0-1范围。

### 4.1.2 逻辑斯蒂回归

1. **逻辑斯蒂回归（Logistic regression）**可以看成是线性回归的推广（广义线性回归），针对2分类问题，是神经网络中重要部分（激活函数，Sigmoid）。其形式为：
   $$
   p(X)=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}} \tag{4.1}
   $$
   <img src="images\ch4\4-2.png" style="zoom:40%;" />

   当概率超过阈值时，则为正类，小于阈值时为负类。

2. 4.1式可整理为$\frac{p(X)}{1-p(X)}=e^{\beta_0+\beta_1X}$，其中$\frac{p(X)}{1-p(X)}$称为**几率（odd）**，取值范围为0到$\infty$，对其两边取对数可得$log(\frac{p(X)}{1-p(X)})=\beta_0+\beta_1X$，因此，逻辑斯蒂回归可以视为分对数变换下关于X的线性回归模型，逻辑斯蒂模型也较为**对数几率回归**。（参考《机器学习》—周志华）

3. 逻辑斯蒂回归**对Y属于某一类的概率建模**，而不直接对响应变量Y建模。

4. 估计回归系数可使用极大似然估计，似然函数为：
   $$
   L(\beta_0,\beta_1)=\prod _{i:y_i=1}p(x_i)\prod _{i':y_{i'}=0}(1-p(x_{i'}))
   $$

   > 推导：
   > $$
   > \begin{align}
   > 似然函数：L(w)&=\prod_{i=1}^N[p(x_i)]^{y_i}[1-p(x_i)]^{1-y_i}\\
   > 对数似然函数：\mathbb{L}&=\sum_{i=1}^{N}[y_ilog\ p(x_i)+(1-y_i)log(1-p(x_i))]\\
   > &=\sum_{i=1}^{N}[y_ilog\ \frac{p(x_i)}{1-p(x_i)}+log(1-p(x_i))]\\
   > &=\sum_{i=1}^{N}[y_i(w\cdot x_i)-log(1+e^{(w\cdot x_i)})]\\
   > \end{align}
   > $$
   > 使用梯度下降算法或牛顿法求解对数似然函数的极大值

   > 注：极大似然估计可参考另一个笔记《概率论与数理统计笔记》https://github.com/QianXzhen/Statistics-note

### 4.1.3 线性判别分析和二次判别分析（LDA和QDA）

> 注：本书中是以统计（贝叶斯决策理论）角度来阐释的，从线性空间角度可以参考《机器学习》—周志华

1. **贝叶斯定理（Bayes theorem）**可以表述为
   $$
   p_k(X)=\frac{\pi_kf_k(x)}{\sum^K_{l=1}\pi_lf_l(x)}
   $$
   其中，$p_k(X)=P(Y=k|X=x)$表示$X=x$的观测属于第$k$类的后验概率，$f_k(X)=P(X=x|Y=k)$表示第$k$类观测的$X$的密度函数，$\pi_k$为一个随机选择的观测来自$k$类的先验概率。**贝叶斯分类起将观测分到$p_k(X)$最大的一类中。**

2. **单预测变量线性判别分析**，假设$f_k(x)$是**正态的或高斯的**，一维情况其密度函数为
   $$
   f_k(x)=\frac{1}{\sqrt{2\pi}\sigma_k}\exp{(-\frac{1}{2\sigma^2_k}(x-\mu_k)^2)}
   $$
   其中，$\mu_k$和$\sigma^2_k$是第$k$类的平均值和方差，且假设所有**方差是相等**的，则
   $$
   p_k(x)=\frac{\pi_k\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2\sigma^2}(x-\mu_l)^2)}
   $$
   化简得到等价式
   $$
   \delta_k(x)=x\cdot \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log(\pi_k)
   $$
   > 证明：
   > $$
   > \begin{align}
   > &\left\{\begin{aligned}
   > p_k(x) &= \frac {\pi_k
   >                 \frac {1} {\sqrt{2 \pi} \sigma}
   >                 \exp(- \frac {1} {2 \sigma^2} (x - \mu_k)^2)
   >                }
   >                {\sum {
   >                 \pi_l
   >                 \frac {1} {\sqrt{2 \pi} \sigma}
   >                 \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
   >                }}\\
   > \delta_k(x) &= x \frac {\mu_k} {\sigma^2} - \frac {\mu_k^2} {2 \sigma^2}
   >               + \log(\pi_k)\\
   > \end{aligned}\right.\\
   > 
   > &令c = \frac {
   >                 \frac {1} {\sqrt{2 \pi} \sigma}
   >                 \exp(- \frac {1} {2 \sigma^2} x^2)
   >                }
   >                {\sum {
   >                 \pi_l
   >                 \frac {1} {\sqrt{2 \pi} \sigma}
   >                 \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2)
   >                }}\\
   > 
   > &\therefore p_k(x) = C \pi_k \exp(- \frac {1} {2 \sigma^2} (\mu_k^2 - 2x \mu_k))\\
   > &\therefore log(p_k(x)) = log(C) + log(\pi_k) + (- \frac {1} {2 \sigma^2} (\mu_k^2 - 2x \mu_k))\\
   > &\therefore log(p_k(x)) =  (\frac {2x \mu_k} {2 \sigma^2} -\frac {\mu_k^2} {2 \sigma^2}) + log(\pi_k) + log(C)\\
   > &\because C不随着k的变化而改变，其值是一个定值\\
   > &\therefore 令\delta_k(x)=(\frac {2x \mu_k} {2 \sigma^2} -\frac {\mu_k^2} {2 \sigma^2}) + log(\pi_k)，最大化p_k(x)等价最大化\delta_k(x)
   > \end{align}
   > $$
   
   对于贝叶斯分类只要$\delta_k(x)$达到最大即可。与之相同，因为并不知道原始的参数，需要通过样本估计$\mu_k$、$\sigma$和$\pi_k$，其中
$$
\begin{align}
   \hat{\mu}_k&=\frac{1}{n_k}\sum_{i:y_i=k}x_i\\
   \hat{\sigma}^2&=\frac{1}{n-K}\sum_{k=1}^K \sum_{i:y_i=k}(x_i-\hat{\mu}_k)^2\\
   \hat{\pi}_k&=\frac{n_k}{n}
   \end{align}
$$
​		将这些估计值代入$\delta_k(x)$中，得到$\hat{\delta}_k(x)$，找到令其值最大的类别$k$作为观测的预测类别。

3. **多预测变量线性判别分析**，其原理和但预测变量相同，但是基于多元预测变量，所以均值和方差都变成了**均值向量**、**协方差矩阵**，多元高斯分布密度函数形式为
   $$
   f(x)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp{(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}
   $$
   $\delta_k(x)$形式为
   $$
   \delta_k(x)=x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k+\log\pi_k
   $$

4. **二次判别分析**与LDA不同之处在于不假设各样本的协方差矩阵（方差）相等，每类观测都有自己的协方差矩阵，形式为
   $$
   \delta_k(x)=-\frac{1}{2}x^T\Sigma_k^{-1}x+x^T\Sigma_k^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k+\log\pi_k
   $$
   QDA和LDA的关系如同非线性回归和线性回归的关系，归属于方差和偏差权衡的问题。

### 4.1.4 ROC曲线

> 参考知乎回答：https://www.zhihu.com/question/39840928/answer/241440370（作者：无涯）

1. 混淆矩阵中有着Positive、Negative、True、False的概念，其意义如下：

   - 称预测类别为1的为Positive（阳性），预测类别为0的为Negative（阴性）。
   - 预测正确的为True（真），预测错误的为False（伪）。

   <img src="images\ch4\confuse.png" style="zoom:100%;" />

**然后**，由此引出True Positive Rate（真阳率）、False Positive（伪阳率）两个概念：

* $TP=\frac{TP}{TP+FN}$，指所有真实类别为1的样本中，预测类别为1的比例；
* $FP=\frac{FP}{FP+TN}$，指所有真实类别为0的样本中，预测类别为1的比例。

2. ROC曲线的横轴是FPRate，纵轴是TPRate，当二者相等时，即y=x，如下图

   <img src="images\ch4\ROC.png" style="zoom:100%;" />

   一个理想的ROC曲线会紧贴左上角，即期望真阳率为1，假阳率为0.

3. AUC（area under the ROC）是ROC曲线下的面积，其最小值为0.5，即上图的面积，一个理想的ROC曲线的AUC为1，AUC的优势是**AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价**。

   例子：

   例如在反欺诈场景，设欺诈类样本为正例，正例占比很少（假设0.1%），如果使用准确率评估，把所有的样本预测为负例，便可以获得**99.9%的准确率**。

   但是如果使用AUC，把所有样本预测为负例，TPRate和FPRate同时为0（没有Positive），与(0,0) (1,1)连接，得出**AUC仅为0.5**，成功规避了样本不均匀带来的问题。

## 4.2 题目答案

1. 证明逻辑斯蒂函数表达式和分对数表达式等价
   $$
   \begin{align}
   &\left\{\begin{aligned}
   P(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\\
   \frac {p(X)} {1 - p(X)} =e^{\beta_0 + \beta_1 X}\\
   \end{aligned}\right.\\
   
   下证：\\
   \frac {p(X)} {1 - p(X)}&= \frac {\frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}
           {1 - \frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}
   \\
   \\
   &= \frac {\frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}
           {
             \frac {1 + e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}
             - \frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}
           }
   \\
   \\
   &= \frac {\frac {e^{\beta_0 + \beta_1 X}} {1 + e^{\beta_0 + \beta_1 X}}}
           {\frac {1} {1 + e^{\beta_0 + \beta_1 X}}}\\
   \\
   &=e^{\beta_0 + \beta_1 X}\\
   \end{align}
   $$
   
2. 贝叶斯将观测分入最大概率类别中$p_k(x)$和$\delta_k(x)$等价，已证。

3. 设一类观测服从均值向量不同、协方差矩阵不等的正态分布，考虑只有一元变量，有K类观测，证明该种情况下，贝叶斯分类起不是线性的，是二次的。
   $$
   p_k(x) = \frac {\pi_k \frac {1} {\sqrt{2 \pi} \sigma_k} \exp(- \frac {1} {2 \sigma_k^2} (x - \mu_k)^2) } {\sum { \pi_l \frac {1} {\sqrt{2 \pi} \sigma_l} \exp(- \frac {1} {2 \sigma_l^2} (x - \mu_l)^2) }}\\
   令C' = \frac { \frac {1} {\sqrt{2 \pi}}} {\sum { \pi_l \frac {1} {\sqrt{2 \pi} \sigma_k} \exp(- \frac {1} {2 \sigma_k^2} (x - \mu_l)^2) }}\\
   \therefore p_k(x) = C' \frac{\pi_k}{\sigma_k} \exp(- \frac {1} {2 \sigma_k^2} (x - \mu_k)^2)\\
   \therefore log(p_k(x)) = log(C') + log(\pi_k) - log(\sigma_k) + (- \frac {1} {2 \sigma_k^2} (x - \mu_k)^2)\\
   $$
   所以$log(p_k(x))$是关于$x$的二次函数。

4. 当变量维数$p$很大时，只用测试观测附近的观测去做预测的局部方法效果都不理想，这种现象称为**维数灾难**（curse of dimensionality），即当$p$很大时，非参数模型效果很差。
5. LDA v.s. QDA
   * 如果**贝叶斯决策边界是线性**的，则训练集上QDA比LDA的效果好，测试集上LDA比QDA的效果好；
   * 如果**贝叶斯决策边界是非线性**的，则训练集和测试集上QDA比LDA的效果好；
   * 在一般情况下，随着样本量$n$增大，相比于LDA的测试预测率，QDA的预测率将变得更好，因为较大的样本量可以抵消方差，避免过拟合；
   * 如果贝叶斯边界是线性的，应该使用LDA，不能因为QDA的光滑度高而选用。

6-9. 略

# 第五章 重抽样方法

&emsp;&emsp;重抽样（resampling）通过反复从训练集中抽取样本，然后根据每一个样本重新拟合一个新模型，以此获得该模型的评价信息。重采样方法选择模型时，不是为了完美地估计出真实测试错误率，而是选择较优的超参数，使得模型能达到或逼近最低测试错误率。

## 5.1 相关概念

### 5.1.1 交叉验证法（Cross Validation）

1. **验证集法（validation set approach）**，将获得的观测数据分为两个大小相当的子集：**训练集（training set）**和**验证集（validation set）**，或者说**保留集（hold-out set）**，其原理下图所示。

   <img src="images\ch5\5-1.png" style="zoom:50%;" />

   验证集方法**优点****原理简单，易于执行，但有两个潜在的**缺陷**：1.模型的测试错误率波动大；2.验证集错误率可能高估真实测试错误率。

2. **留一交叉验证法（leave-one-out cross-validation，LOOCV）**，该方法将当一个单独的观测作为验证集，剩下的n-1个观测组成训练集，重复该步骤n次，最后测试均方误差的LOOCV估计是这n个测试误差估计的均值：$CV_{(n)}=\frac{1}{n}\sum_{i=1}^nMSE_i$，其原理如下图所示。

   <img src="images\ch5\5-2.png" style="zoom:50%;" />

   LOOCV方法有以下几个**优点**：1.估计的偏差小；2.由于LOOCV方法在训练集和验证集的分割上不存在随机性，因此多次运用LOOCV方法总会得到相同的结果。**缺点**：1.计算量很大，模型需要被拟合n次；2产生的测试误差估计的方差比k折CV法的大。

   > 注：用最小二乘法拟合线性或者多项式回归模型时，LOOCV方法所花费的时间将缩减至与拟合一个模型相同，公式如下：
   > $$
   > CV_{(n)}=\frac{1}{n}\sum_{i=1}^n(\frac{y_i-\hat{y}_i}{1-h_i})^2
   > $$
   > 其中$\hat{y}_i$为原始最小二乘拟合的第$i$个拟合值，$h_i$为杠杆值

3. **k折交叉验证法（k-fold CV）**，将观测值随机分为k个大小基本一致的组，或折（fold），选择一折作为验证集，剩下的k-1折作为训练集，重复k次，最后k折CV估计是k个测试误差估计的均值：$CV_{(k)}=\frac{1}{k}\sum_{i=1}^kMSE_i$，其原理如下图所示。

   <img src="images\ch5\5-3.png" style="zoom:50%;" />

   在实践中，一般令k=5或k=10，特别的，当k等于n时，该方法即为LOOCV。k折交叉验证法具有较多优点，最重要的是**该方法能兼顾到方差和偏差的权衡**。

### 5.1.2 自助法（Bootstrap）

&emsp;&emsp;自助法简单的来说就是对现有观测集进行有放回抽样，形成若干个样本，该原理如下图所示。

<img src="images\ch5\5-4.png" style="zoom:50%;" />

> 在Bootstrap中，用替换法从大小为N的数据集中采样N个实例，原始数据集被用作验证集。选择一个实例的概率是$\frac{1}{N}$;我们不选择这个实例的概率$1-\frac{1}{N}$。N次抽样后我们没有选中这个实例的概率是$(1-1/N)^{N} \approx e^{-1}=0.368$。这意味着训练数据只包含约$63.2\%$的实例，也就是说这个系统不会在另外$36.8\%$的数据上训练，这会为评估带来悲观偏差。解决方法是重复这个过程很多次来观察平均值。
>
> 转自：Introduction to Machine Learning（Ethem Alpaydin）

## 5.2 题目答案

1. 证明$\alpha = \frac {\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2 \sigma_{XY}}$使$Var(\alpha X + (1 - \alpha)Y)$最小。
   $$
   \begin{align}
   Var(\alpha X + (1 - \alpha)Y)
   &= Var(\alpha X) + Var((1 - \alpha) Y) + 2 Cov(\alpha X, (1 - \alpha) Y)
   \\
   &= \alpha^2 Var(X) + (1 - \alpha)^2 Var(Y) + 2 \alpha (1 - \alpha) Cov(X, Y)
   \\
   &= \sigma_X^2 \alpha^2 + \sigma_Y^2 (1 - \alpha)^2 + 2 \sigma_{XY} (-\alpha^2 +
   \alpha)\\
   &=(\sigma_X^2+\sigma_Y^2-2\sigma_{XY})\alpha^2+(2\sigma_{XY}-2\sigma_Y^2)\alpha+\sigma_Y^2\\
   \therefore argmin(Var(aX+(1-\alpha)Y))&=-\frac{2\sigma_{XY}-2\sigma_Y^2}{2(\sigma_X^2+\sigma_Y^2-2\sigma_{XY})}
   \end{align}
   $$
   
2. 见5.1.2
3. 4. 略

# 第六章 线性模型选择与正则化

&emsp;&emsp;本章讨论的是第3章中线性回归模型方法的拓展和改进，但其中涉及的概念同样适用于其他方法，例如第4章中介绍的分类模型。

## 6.1 相关概念

### 6.1.1 子集选择

1. **最优子集选择（best subset selection）**，即对p个预测变量的所有可能组合都建立模型，在所有模型中选择最优的一个，算法属于**暴力搜索法**，算法步骤如下：

   >**最优子集选择算法**
   >
   >* 记不含预测变量的零模型为$M_0$，只用于估计各观测的样本均值；
   >* 对于$k=1,2,...,p$：
   >  * 拟合$\binom{p}{k}$个包含$k$个预测变量的模型；
   >  * 在$\binom{p}{k}$个模型中选择RSS最小或$R^2$最大的作为最优模型，记为$M_k$。
   >* 根据交叉验证预测误差、$C_p(AIC)$、$BIC$或者调整$R^2$从$M_0,...,M_p$个模型中选择一个最优模型。

   该算法需要拟合$2^n$个模型，在模型的选取中，上述算法步骤II中，**将$2^n$个模型的最优选择转换为了从$p+1$个备选模型选择的问题**，即<u>先利用训练误差（RSS，$R^2$）在同等维度的模型选一个，再利用测试误差（或者可以是测试误差的无偏估计）在不同维度的模型中选一个</u>。

   该方法**可以选择最优的模型**，但是由于需要拟合$2^n$个模型，导致**计算效率低**。

2. **逐步选择（stepwise selection）**相比较于最优子集选择方法，该方法基于**贪婪思想**，选择后的模型逼近最优模型（很难找到最优模型），方法分为3种：**向前逐步选择（forward stepwise selection）**、**向后逐步选择（backward stepwise selection）**和**混合方法**。这类方法可以将模型拟合的次数转换为$\sum^{p-1}_{k=0}(p-k)=1+p(p+1)/2$。以上三次算法的大致思想在3.1.2.7中已经介绍，下面将总结这三类算法的具体步骤：

   > **向前逐步选择算法**
   >
   > * 记不含预测变量的零模型为$M_0$；
   > * 对于$k=0,1,2,...,p-1$：
   >   * 从$p-k$个模型中进行选择，每个模型都在模型$M_k$的基础上增加一个变量；
   >   * 在$p-k$个模型中选择RSS最小或$R^2$最高的模型作为最优模型，记为$M_{k+1}$。
   > * 根据交叉验证预测误差、$C_p(AIC)$、$BIC$或者调整$R^2$从$M_0,...,M_p$个模型中选择一个最优模型。

   > **向后逐步选择算法**
   >
   > * 记包含全部$p$个预测变量的全模型为$M_p$；
   > * 对于$k=p,p-1,...,1$：
   >   * 从$k$个模型中进行选择，在模型$M_k$的基础上减少一个变量，则模型只含$k-1$个变量；
   >   * 在$k$个模型中选择RSS最小或$R^2$最高的模型作为最优模型，记为$M_{k-1}$。
   > * 根据交叉验证预测误差、$C_p(AIC)$、$BIC$或者调整$R^2$从$M_0,...,M_p$个模型中选择一个最优模型。
   
   > **混合方法**
   >
   > 将向前和向后逐步选择进行结合，与向前逐步选择类似，该方法逐次将变量加入模型中，然而**在加入新变量的同时，该方法也移除不能提升模型拟合效果的变量**。

3. 在以上所有算法的最后一步都是基于**测试误差**的，因此RSS和$R^2$不适用。为了达到基于测试误差选择最优模型的目的，**需要估计测试误差**，通常有两种方法：

   * 根据过拟合导致的偏差对训练误差进行调整，**间接地估计测试误差**；
   * 通过验证集方法或交叉验证方法，**直接估计测试误差**。

4. 间接估计测试误差的方法主要有：$C_p$、**赤池信息量准则（Akaike information criterion, AIC）**、**贝叶斯信息准则（Bayesian information criterion, BIC）**与**调整$R^2$（adjusted $R^2$）**

   * $C_p=\frac{1}{n}(RSS+2d\hat{\sigma}^2)$，其中$\hat{\sigma}^2$是各个响应变量观测误差的方差$\sigma$的估计值，$C_p$统计量在训练集RSS的基础上增加惩罚项$2d\hat{\sigma}^2$，用于调整训练误差倾向于低估测试误差的现象。

     > Mallow的$C_p$定义为$C'_p=RSS/\hat{\sigma}^2+2d-n$，上式的$C_p$与$C'_p$等价，$C_p=\hat{\sigma}^2(C'_p+n)$，因此具有最小$C_p$值的模型也有最小的$C'_p$值

   * AIC准则适用于许多使用极大似然法进行拟合的模型，$AIC=\frac{1}{n\hat{\sigma}^2}(RSS+2d\hat{\sigma}^2)$，**AIC与$C_p$彼此成比例**。

   * BIC是从贝叶斯观点中衍生出来的，与$C_p$（及AIC）准则十分相似，$BIC=\frac{1}{n}(RSS+log(n)d\hat{\sigma}^2)$，**BIC统计量通常给包含多个变量的模型施以较重的惩罚，故而与$C_p$相比，得到的模型规模更小**。

   * **$R^2$随着模型包含的变量个数的增加而增加**，调整$R^2$可以避免这种情况，$调整R^2=1-\frac{RSS/(n-d-1)}{TSS/(n-1)}$，与其他准则不同，调整$R^2$的值越大，模型测试误差越低。

     > 调整$R^2$背后的想法是当模型包含了所有正确的变量，再增加其他冗余变量只会导致RSS小幅度的减小。在理论上，拥有最大调整$R^2$的模型只包含了正确的变量，而没有冗余变量，与$R^2$不同，调整后的$R^2$对纳入不必要变量的模型引入了惩罚。

### 6.1.2 压缩估计方法

1. **岭回归（ridge regression）**与最小二乘十分相似，该方法添加了**L2正则项**，即最小化下式：
   $$
   \sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_{ij})^2+\lambda\sum^p_{j=1}\beta_j^2=RSS+\lambda\sum^p_{j=1}\beta_j^2
   $$
   其中，$\lambda\geq0$为调节参数，需要使用交叉验证选取最优值，$\lambda\sum^p_{j=1}\beta_j^2$为L2正则项，称为压缩惩罚。**岭回归可以将回归系数的估计值压缩至零，$\lambda$越大，压缩比例越大（不对截距系数（偏置）压缩）。**其压缩结果如下图所示，当$\lambda$越来越大时，各系数趋于0.

   <img src="images\ch6\6-1.png" style="zoom:50%;" />

   > 注：岭回归与普通线性回归不同，后者无论第$j$个变量如何按比例变化，$X_j\hat{\beta}_j$保持不变，而岭回归$X_j\hat{\beta}_j$不仅取决于$\lambda$，还取决于第$j$个预测变量的尺度，甚至受其他预测变量尺度影响，因此在**做岭回归之前，需要对各个预测变量标准化**。

   **优：**与最小二乘法相比，岭回归综合权衡了误差与方差。**缺：**面对对模型没有影响（贡献）的变量，岭回归不会将它们剔除，仅仅将其系数向0方向缩减，所以岭回归最后的**模型仍然是一个全模型**。

2. **Lasso（least absolute shrinkage and selection operator）**

   与岭回归相比，Lasso基于**L1正则项**，在将回归系数压缩的同时，也对无用（无贡献）的预测变量进行剔除，Lasso的系数需要最小化下式：
   $$
   \sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_{ij})^2+\lambda\sum^p_{j=1}｜\beta_j｜=RSS+\lambda\sum^p_{j=1}｜\beta_j｜
   $$
   其中，$\lambda\sum^p_{j=1}｜\beta_j｜$称为L1正则项，当调节参数$\lambda$足够大时，L1惩罚项具有将其中某些系数的估计值强制设定为0的作用，得到了**稀疏模型（sparse model）**。**因此Lasso建立的模型比岭回归建立的模型相比更易于解释。**其压缩结果如下图所示，和岭回归一样，当$\lambda=0$时，与最小二乘等价，当$\lambda\rightarrow \infty$时，得到零模型。不同的是在两个极端之间，得到稀疏模型。

   <img src="images\ch6\6-2.png" style="zoom:50%;" />

3. **岭回归和Lasso对比**

   * Lasso和岭回归的系数估计等价于优化下列问题：
     $$
     \mathop{minimize}\limits_{\beta}\{\sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_{ij})^2\},\quad\sum_{j=1}^p|\beta_j|\leq s\\
     \mathop{minimize}\limits_{\beta}\{\sum^n_{i=1}(y_i-\beta_0-\sum^p_{j=1}\beta_jx_{ij})^2\},\quad\sum_{j=1}^p\beta_j^2\leq s
     $$
     以$p=2$为例，上式分别表示在一个菱形区域和圆形区域的约束下进行参数估计，如下图所示。

     <img src="images\ch6\6-4.png" style="zoom:35%;" />

     上图也解释了为什么Lasso既可以压缩系数，也可以选择变量子集，而岭回归只可以压缩系数，这是因为**岭回归的条件区域是没有尖点的圆形，所以相交点一般不会出现在坐标轴上**，而**Lasso的条件区域在每个坐标轴上都有拐角，所以椭圆经常在坐标轴上与条件区域相交**。

   * Lasso较岭回归具有较大优势，但两者那个精度高，或者效果好呢？根据**没有免费的午餐**定律，需要基于不同情形讨论，简单的来说：**当所有预测变量都对响应变量有贡献时，岭回归效果较好**；**当只有部分预测变量对响应变量有贡献时，Lasso效果较好**。

   * 两类方法比较于最小二乘时，如下图所示，**岭回归以相同比例 压缩每个维度**，而**Lasso中每个系数以$\lambda/2$为阈值压缩至零，即绝对值小于$\lambda/2$的系数被压缩至零，其他系数按比例压缩**。

     <img src="images\ch6\6-5.png" style="zoom:50%;" />

### 6.1.3 降维方法

1. 降维通过将预测变量进行转换，即将$X_1,...,X_p$变换成$Z_1,...,Z_M$，其中$Z_m$表示原始预测变量的线性组合（$M<p$），如下式所示：
   $$
   Z_m=\sum^p_{j=1}\phi_{jm}X_j
   $$
   其中，$\phi_{jm}$是常数。可以通过这些变换后的变量拟合模型。

2. **主成分回归（principal components regression，PCR）**基于**主成分分析（principal components analysis，PCA）**算法对各预测变量进行降维处理，是一种**无监督式**的方法。主成分回归遵循**将预测变量投影后观测值尽可能分散且各主成分互相正交**的原则，以第一主成分为例，该方向上数据的波动性最大，即投影的方差最大，而投影到其他方向的方差比这个小，如下图所示。

   <img src="images\ch6\6-6.png" style="zoom:50%;" />

   由于**少数的主成分足以解释大部分的数据波动和数据与响应变量之间的关系**，如果$Y$的方向就是$X_1,...,X_p$变动最为剧烈的方向，那么用$Z_1,...,Z_M$拟合一个最小二乘模型的结果要优于用$X_1,...,X_p$拟合的结果，估计$M\ll p$个系数会**减轻过拟合**。**主成分回归与岭回归非常相似，岭回归可以认为是连续型的主成分回归，因此主成分回归不适用于只包含特征变量子集的模型。**

   > 注：
   >
   > * 主成分数量一般通过交叉验证确定；
   >
   > * 在构造主成分前，需要对各变量做标准化处理，因为方差较大的变量将在主成分中占主导地位。
   >
   > * PCR的降维过程中由于没有响应变量的参与，因此无法保证那些很好地解释预测变量的方向同样可以很好预测响应变量。
   >
   > PCA原理将在第10章进行详细介绍，感兴趣可以提前参考下面的博客，讲得非常细致入微：http://blog.codinglabs.org/articles/pca-tutorial.html

3. **偏最小二乘（partial least squares，PLS）**是一种监督式的主成分回归替代方法，同PCR一样，PLS是一种**降维**手段。以第一个偏最小二乘方向$Z_1$为例，偏最小二乘法将$\phi_{j1}$设定为$Y$对$X_j$**简单线性回归的系数**，因此偏最小二乘法将最大权重赋给与响应变量相关性最强的变量。为确定第二个偏最小二乘方向，需对$Z_1$每个变量做回归，计算残差，再根据正交性计算$Z_2$，往后的最小二乘方向同理。

   PLS方向不仅可以像PCA一样很好地拟合预测变量，而且更好地解释了响应变量。同PCA一样，PLS的方向个数需要通过交叉验证选择，PLS虽然可以减小偏差，但可能同时增大方差，所以PLS和PCA需要的适用性分情况讨论。

### 6.1.4 高维问题

1. 大部分用于回归和分类的传统统计方法是在低维情况下发展而来，也就是观测数$n$远远大于特征数$p$。**当$p\geq n$即特征数比观测数大时，该数据被称为高维数据**，此时像最小二乘法这样传统的方法已不再适用。

   > 注：$p$可以很大，但是观测数$n$常常由于成本、抽样能力和其他因素受到限制。

2. 在高维数据的建模中，最常出现的问题就是**过拟合**。<u>以最小二乘法为例，不管特征变量和响应变量是否真正存在关系，最小二乘估计的系数都能很好地拟合数据，使得模型的残差为零，</u>如下图所示。

   <img src="images\ch6\6-7.png" style="zoom:40%;" />

   当特征变量变多时，即使这些变量与响应变量不相关，但**模型的$R^2$随着预测变量个数的增加而增长到1**，**相应的训练数据集均方误差降为0**，**独立测试集上的均方误差将变得非常大**，如下图所示。

   <img src="images\ch6\6-8.png" style="zoom:50%;" />

   **由于训练均方误差为0，$\hat{\sigma}^2=0$，$C_p$、AIC和BIC将不再适用。**

3. 由于上述高维数据导致的问题，传统回归分析已不再适用，本章介绍的**向前逐步选择、岭回归、lasso和主成分回归等将发挥重大作用**，这些回归算法避免了过拟合问题。在高维数据的建模中，有三个注意点：

   * 正则或压缩在高维问题中至关重要；
   * 合适的调节参数对于得到好的预测结果非常关键；
   * 测试误差随着数据维度（即特征或预测变量的个数）的增加而增大，除非新增的特征变量与响应变量缺失有关。（**维数灾难**）

   > 那些允许模型引入成千上万个特征变量的新方法就是一把**双刃剑**：如果特征变量与问题确实相关，那么它们将提升预测模型，否则将导致更糟的结果。

4. 在高维情况下，存在非常极端的**共线性**：模型中的任何一个变量都可以写成其他变量的线性组合。

5. 当$p>n$时容易得到一个残差为零蛋没有用的模型，因此**不能在训练集上用误差平方、p值、$R^2$统计量或者其他传统的度量方法来检测拟合的效果**，取而代之，更为重要的是**在独立的测试集上验证或者进行交叉验证**。

## 6.2 题目答案

1. 在单个数据集上使用最优子集选择、向前逐步选择和向后逐步选择。对于每种方法，可获得$p+1$个模型，每个模型包含$0,1,2,...,p$个预测变量。

   * 含有$k$个预测变量的三个模型中，**最优子集将具有最小的训练RSS**，因为模型将在训练RSS上进行优化，并且最佳子集已经PK掉了正向和反向选择将尝试的每个模型。
   * 含有$k$个预测变量的三个模型中，**最好的测试RSS模型可以是这三种模型中的任何一种**。如果数据相对于n个观测值具有较大的p个预测变量，则最优子集可能容易过拟合。向前和向后选择可能不会收敛于同一模型，但会尝试使用相同数量的模型，并且很难说哪种选择过程会更好。
   * 使用向前/后逐步选择法选取的$k$变量模型中的预测变量是使用向前/后逐步选择法选取的（$k+1$）变量模型中预测变量的子集。

2. * 与最小二乘相比，lasso和岭回归**灵活性（flexible）更差**，并且当模型预测结果的偏差增大的大小小于其方差减小的大小时，模型给出的预测值更准确。
   * 与最小二乘相比，非线性**灵活性（flexible）更好**，并且当模型预测结果的方差增大的大小小于其偏差减小的大小时，模型给出的预测值更准确。

3. 假设通过最小化以下式子来估计一个线性回归模型中的回归系数：
   $$
   \sum^n_{i=1}(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij}),\quad \sum_{j=1}^p|\beta_j|\leq s
   $$
   s为某给定值，下面的几个定论均为正确：

   * 随着s从0开始增加，训练集RSS会稳定减小。（s增加时，代表约束减小，模型使用的特征变多，进而容易过拟合）
   * 随着s从0开始增加，测试集RSS会最初减小，然后开始增加，图像呈现一个U形。（s=0代表刚开始是个零模型，模型处于欠拟合，然后逐渐转为过拟合，偏差减小，方差变大）
   * 随着s从0开始增加，方差稳定增长。（同训练集RSS，处于过拟合）
   * 随着s从0开始增加，偏差稳定减小。（同上，处于过拟合）
   * 随着s从0开始增加，不可约误差保持不变。（不可约误差为常数，不被模型影响）

4. 同3，略

5-7. 略（不会）

# 第七章 非线性模型

## 7.1 相关概念

### 7.1.1 多项式回归

&emsp;&emsp;多项式回归是线性模型的一个非线性推广，是一个全局的模型，形式为：
$$
y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+...+\beta_dx_i^d+\epsilon_i
$$
其中，$\epsilon_i$是误差项。上式也可看成**预测变量$x_i,\ x_i^2,\ x_i^3,\ ...,\ x_i^d$的标准线性模型，因此可以使用最小二乘法进行参数估计**。

> 注：多项式阶数$d$的选择不宜过大，**一般不大于3或4**，当$d$越大时，多项式曲线就会变得越光滑，在$X$变量定义域边界处呈现异样形状。

### 7.1.2 阶梯函数

&emsp;&emsp;阶梯函数非全局，该模型**将$X$的取值范围分为一些区间**，**每个区间拟合一个不同的常数**。具体地，在$X$取值空间上创建分割点$c_1,\ c_2,\ ...,\ c_K$，然后构造$K+1$个新变量：
$$
\begin{align}
C_0(X)&=I(X<c_1)\\
C_1(X)&=I(c_1\leq X<c_2)\\
C_2(X)&=I(c_2\leq X<c_3)\\
& \vdots\\
C_{K-1}(X)&=I(c_{K-1}\leq X<c_K)\\
C_K(X)&=I(c_K\leq X)
\end{align}
$$
其中，$I(\cdot)$是示性函数，当条件成立时返回1否则返回0.因为$X$只能落在一个区间内，所以$C_0(X)+C_1(X)+...+C_K(X)=1$，以该变量作为预测变量，构建模型：
$$
y_i=\beta_0+\beta_1C_1(x_i)+\beta_2C_2(x_i )+...+\beta_KC_K(x_i)+\epsilon_i
$$

> 注：当$X<c_1$时，上式每个预测变量都为零，所以$\beta_0$为$X<c_1$时$Y$的平均值。相应的，当$c_j<X<c_{j+1}$时，预测值为$\beta_0+\beta_j$，这样$\beta_j$则被解释为当$X$由$X<c_1$增至$c_j<X<c_{j+1}$时响应变量的平均增量。

&emsp;&emsp;如果预测变量不具有明显的分割点，那么分段固定值拟合就十分不恰当，如下图所示。

<img src="images\ch7\7-1.png" style="zoom:50%;" />

### 7.1.3 基函数

&emsp;&emsp;多项式和阶梯函数实际上都是特殊的基函数，基函数原理是对变量$X$的函数或变换$b_1(X),\ b_2(X),\ ...,\ b_k(X)$进行建模，形式为：
$$
y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i )+...+\beta_Kb_K(x_i)+\epsilon_i
$$

### 7.1.4 回归样条

1. 样条函数是多项式回归和阶梯函数回归的延伸和推广，即**在$X$的不同区域拟合独立的低阶多项式函数**，以此取代$X$在全部取值范围内拟合高阶多项式。系数发生变化的临界点称为**结点（knot）**。以分段三次多项式为例，其在不同区域的拟合形式为：
   $$
   y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+\epsilon_i
   $$
   当结点数为1时，模型形式为：
   $$
   y_i=
   \left\{\begin{aligned}
   \beta_{01}+\beta_{11}x_i+\beta_{21}x_i^2+\beta_{31}x_i^3+\epsilon_i,\quad x_i<c\\
   \beta_{02}+\beta_{12}x_i+\beta_{22}x_i^2+\beta_{32}x_i^3+\epsilon_i,\quad x_i\geq c
   \end{aligned}\right.\\
   $$
   即模型拟合两个不同的多项式函数，其中有个在$x_i<c$的子集上，另一个在$x_i\geq c$的子集上。**由于每个模型有四个参数，所以共使用了8个自由度构建该分段多项式模型**。

2. 由于结点的存在，如果对**结点部分不加以约束**，将会出现下图左上角情况，即函数不连续。当添加了**确保拟合曲线是连续**的约束条件，图像则是右上角情况，但仍会出现尖点。当在**结点处添加一阶导数和二阶导数都是连续**的约束条件，如左下角情况，结点处是光滑的。值得注意的是，**每个对分段三次多项式施加的约束都有效地释放了一个自由度**，减少模型的复杂性。如左下图施加了三个约束（连续性、一阶导数的连续性、二阶导数的连续性），将有5个约束，一般情况下**$K$个结点的三次样条会产生$4+K$个自由度**。

   <img src="images\ch7\7-2.png" style="zoom:50%;" />

3. 上述的回归样条略显复杂，需要保证多项式自身以及其前$d-1$阶导数是连续的约束，可以基于基函数模型来表示这样的回归样条，选择合适的基函数表示成如下形式：
   $$
   y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i )+...+\beta_{K+3}b_{K+3}(x_i)+\epsilon_i
   $$
   对于每个结点可以添加**截断幂基**函数，其形式为：
   $$
   h(x,\xi)=(x-\xi)^3_+=\left\{\begin{aligned}
   &(x-\xi)^3 \quad &x>\xi\\
   &0\quad &否则
   \end{aligned}\right.\\
   $$
   其中，$\xi$是结点。

4. 若对边界不加以约束，其拟合曲线的置信带将相当宽，**自然样条（natural spline）**对边界处添加了约束条件：函数在边界区域应该是线性的（边界区域指的是比最小结点处左侧和最大结点的右侧），如下图红色部分所示，红色的置信区间更窄。

   <img src="images\ch7\7-3.png" style="zoom:50%;" />

5. 样条函数的结点选择不当将引起曲线跌宕起伏，实践证明，**令结点在数据上呈现均匀分布是一种比较有效的结点选择方式**，**前提是需要确定所需的自由度**。**选择结点数量客观的方法是交叉验证**。样条函数可以在函数$f$变动较快的区域设置结点，在$f$稳定的地方设置较少结点，保证曲线的光滑性。

### 7.1.5 光滑样条

1. 与前面的样条函数不同，光滑样条不需要先设定一些结点，较为特殊的，光滑样条将**所有不同的$x_i$都设为结点**，然后再**调整样条函数的光滑度**，其优化目标为最小化下式：
   $$
   \sum^n_{i=1}(y_i-g(x_i))^2+\lambda\int g''(t)^2dt
   $$
   其中，$\lambda$是一个非负的调节参数，上式的形式为**“损失函数+惩罚项”**，形如岭回归以及lasso，但不同的是，光滑样条的惩罚项是**控制模型光滑度（或者是粗糙度，roughness）**的，因为函数的一阶导衡量斜率，二阶导衡量斜率的变化程度，即粗糙度。当$\lambda=0$时，惩罚项不起作用，函数$g$将在训练数据上做插值，造成过拟合；当$\lambda\rightarrow \infty$时，$g$变得非常平稳，曲线将变为一条直线，实际上是一条最小二乘直线；当$\lambda$适中时，$g$将尽可能地接近训练点同时也比较光滑。

2. 在光滑样条中，每个数据点都作为一个结点将导致自由度太高，因为$\lambda$允许从0增加到$\infty$，实际的自由度（记作$df_\lambda$，也称**有效自由度**）就从n降到2.自由度指的是自由参数的个数，尽管光滑样条有$n$个参数，即名义上的自由度为$n$，**但$n$个自由度被大量限制或者收缩**。有效自由度计算如下：
   $$
   \pmb{\hat{g}}_\lambda=\pmb{S}_\lambda \pmb{y}\\
   df_\lambda=\sum_{i=1}^n\{\pmb{S}_\lambda\}_{ii}
   $$
   其中，$\pmb{\hat{g}}$是在特定$\lambda$下，光滑样条的拟合值向量，上式表明光滑样条的拟合值向量可以写成$n\times n$的矩阵$\pmb{S}_\lambda$乘以响应向量$y$。有效自由度是矩阵$S_\lambda$的对角元素之和。

3. 虽然不需要事先选择结点个数或位置，但需要确定$\lambda$的值，常使用交叉验证法。特别的，当使用留一交叉验证时（LOOCV），代价与与计算一个拟合模型一样：
   $$
   RSS_{cv}(\lambda)=\sum_{i=1}^n(y_i-\hat{g}_\lambda^{(-i)}(x_i))^2=\sum_{i=1}^n[\frac{y_i-\hat{g}_\lambda(x_i)}{1-\{\pmb{S}_\lambda\}_{ii}}]^2
   $$
   其中，$\hat{g}_\lambda^{(-i)}$是剔除观测点$(x_i, y_i)$外的所有训练数据的拟合值。上式的特别之处：**当计算每个点的留一交叉验证结果时，相当于计算每一个点用所有数据的原始拟合值$\hat{g}_\lambda$.**

### 7.1.6 局部回归

&emsp;&emsp;局部回归在对一个目标观测点$x_0$拟合时，**只用到该点附近的训练观测**，如下图所示，蓝色为真实函数$f(x)$的曲线，浅橘黄色使用局部回归拟合得到的$\hat{f}(x)$的曲线。橘黄色的点表示的是橘黄色直线指示的点$x_0$的邻居，黄色钟形区域代表赋值给$x_0$的邻居的权重，**权重随着距$x_0$的距离越远降至微0**.即**$x_0$的拟合值$\hat{f}(x_0)$是通过拟合一个加权线性回归得到的**。

<img src="images\ch7\7-4.png" style="zoom:50%;" />

>**在$X=x_0$处的局部回归模型算法**
>
>1. 选取占所有数据$s=k/n$比例的最靠近$x_0$的数据$x_i$；
>
>2. 对选出的数据点赋予其权重$K_{i0}=K(x_i,x_0)$。**离$x_0$最远的点的权重为0，而最近的点权重最高**。那些没有被选中的数据点的权重为0；
>
>3. 用定义好的权重在$x_i$处拟合加权最小二乘回归，也就是对下式最小化
>   $$
>   \sum^n_{i=1}K_{i0}(y_i-\beta_0-\beta_1x_i)^2
>   $$
>
>4. 根据$\hat{f}(x_0)=\hat{\beta}_0+\hat{\beta}_1x_0$得到$x_0$的拟合结果。

> 局部回归很复杂，比如如何定义权重函数$K$，第3步应该使用什么函数，第1步中间距$s$的定义等。**地理加权回归（GWR）**是该算法的拓展，因此将在后面空间统计的学习加以详细介绍，在此也立个Flag🚩，<u>后期将做一个空间统计的学习笔记</u>。

### 7.1.7 广义可加模型

1. **广义可加模型（generalized additive model，GAM）**基于多个变量$X_1,\ ...,\ X_n$，提供了一种对标准线性模型进行推广的框架，**每一个变量用一个非线性函数替换**，GAM既可以用于响应变量是定性的情形，也可以用于响应变量是定量的情形。

2. 用于回归问题的GAM，该模型可以将多元线性回归模型重新写成如下形式：
   $$
   \begin{align}
   y_i&=\beta_0+\sum^p_{j=1}f_j(x_{ij})+\epsilon_i\\
   &= \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + ... + f_p(x_{ip}) + \epsilon_i
   \end{align}
   $$
   其中，$f_j(x_{ij})$为光滑的非线性函数，是一个**独立的计算单元**。前面介绍的几个模型都可以作为计算单元。

3. 用于分类问题的GAM，该模型基于逻辑斯蒂回归模型，改写成如下形式：
   $$
   log(\frac{p(X)}{1-p(X)})=\beta_0 + f_1(X_1) + f_2(X_2) + ... + f_p(X_p)
   $$

4. GAM的**优点**：

   * GAM可以对每个变量拟合一个非线性函数，避免手动设置每个变量的变形方式；
   * 非线性拟合模型能将响应变量预测得更准；
   * 由于模型可加，可以固定其他变量，看一个变量对响应变量的影响效果，**适合推断**；
   * 针对变量$X_j$的函数$f_j$的光滑性可以通过对自由度的分析得到；

   GAM的**局限**：

   * 模型的形式被限定为可加形式，在多变量的情况下，这类模型会忽略有意义的交互项。需要添加形式为$X_j \times X_k$的交互项式的GAM也能够表达交互效应。

## 7.2 题目答案

1. (a). $a_1 = \beta_0, b_1 = \beta_1, c_1 = \beta_2, d_1 = \beta_3$.

   (b). 
   $$
   \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x - \xi)^3
   \\
   = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x^3 - 3 x^2 \xi + 3 x \xi^2 - \xi^3)
   \\
   = (\beta_0 - \beta_4 \xi^3) + (\beta_1 + 3 \beta_4 \xi^2) x + (\beta_2 - 3 \beta_4 \xi) x^2 + (\beta_3 + \beta_4) x^3\\
   
   \therefore a_2 = \beta_0 - \beta_4 \xi^3, b_2 = \beta_1 + 3 \beta_4 \xi^2, c_2 = \beta_2 - 3 \beta_4 \xi, d_2 = \beta_3 + \beta_4
   $$
   (c).
   $$
   f_1(\xi) = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3
   \\
   f_2(\xi) = (\beta_0 - \beta_4 \xi^3) + (\beta_1 + 3 \beta_4 \xi^2) \xi + (\beta_2 - 3 \beta_4 \xi) \xi^2 + (\beta_3 + \beta_4) \xi^3
   \\
   = \beta_0 - \beta_4 \xi^3 + \beta_1 \xi + 3 \beta_4 \xi^3 + \beta_2 \xi^2 - 3 \beta_4 \xi^3 + \beta_3 \xi^3 + \beta_4 \xi^3
   \\
   = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + 3 \beta_4 \xi^3 - 3 \beta_4 \xi^3 + \beta_3 \xi^3 + \beta_4 \xi^3 - \beta_4 \xi^3
   \\
   = \beta_0 + \beta_1 \xi + \beta_2 \xi^2 + \beta_3 \xi^3
   $$
   (d).
   $$
   f'(x) = b_1 + 2 c_1 x + 3 d_1 x^2 
   \\
   f_1'(\xi) = \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2
   \\
   f_2'(\xi) = \beta_1 + 3 \beta_4 \xi^2 + 2 (\beta_2 - 3 \beta_4 \xi) \xi + 3 (\beta_3 + \beta_4) \xi^2
   \\
   = \beta_1 + 3 \beta_4 \xi^2 + 2 \beta_2 \xi - 6 \beta_4 \xi^2 + 3 \beta_3 \xi^2 + 3 \beta_4 \xi^2
   \\
   = \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2 + 3 \beta_4 \xi^2 + 3 \beta_4 \xi^2 - 6 \beta_4 \xi^2 
   \\
   = \beta_1 + 2 \beta_2 \xi + 3 \beta_3 \xi^2
   $$
   (e). 
   $$
   f''(x) = 2 c_1 + 6 d_1 x
   \\
   f_1''(\xi) = 2 \beta_2 + 6 \beta_3 \xi
   \\
   f_2''(\xi) = 2 (\beta_2 - 3 \beta_4 \xi) + 6 (\beta_3 + \beta_4) \xi
   \\
   = 2 \beta_2 + 6 \beta_3 \xi
   $$

2. 假设曲线$\hat{g}$是由式子$\hat{g}=\arg\ \mathop{\min}\limits_{g}(\sum^n_{i=1}(y_i-g(x_i))^2+\lambda\int[g^{(m)}(x)]^2dx)$光滑拟合$n$个点得到的，其中$g^{(m)}$是$g$的第$m$阶导数（$g^{(0)}=g$）。请举例说明如下情形下的$\hat{g}$。

   (a).$\lambda=\infty,m=0$

   左边项将没有意义，$\hat{g}=0$，恒为0

   (b).$\lambda=\infty,m=1$

   左边项将没有意义，$\hat{g}=k$，为常数

   (c).$\lambda=\infty,m=2$

   左边项将没有意义，$\hat{g}=ax+b$，为直线

   (d).$\lambda=\infty,m=3$

   左边项将没有意义，$\hat{g}=ax^2+bx+c$，为二次曲线

3. 略

4. 略

5. 考虑通过以下式子定义的两条曲线$\hat{g}_1$，$\hat{g}_2$
   $$
   \hat{g}_1=\arg\ \mathop{\min}\limits_{g}(\sum^n_{i=1}(y_i-g(x_i))^2+\lambda\int[g^{(3)}(x)]^2dx)\\
   \hat{g}_2=\arg\ \mathop{\min}\limits_{g}(\sum^n_{i=1}(y_i-g(x_i))^2+\lambda\int[g^{(4)}(x)]^2dx)
   $$
   其中$g^{(m)}$表示$g$的第$m$阶导数。

   (a)当$\lambda\rightarrow\infty$时，由于$\hat{g}_1$比$\hat{g}_2$的条件更严格，因此$\hat{g}_2$更光滑，所以$\hat{g}_2$的训练RSS更小；

   (b)当$\lambda\rightarrow\infty$时，因为不知道真实的曲线，所以无法评价两者的测试RSS哪个小；

   (c)当$\lambda=0$时，因为惩罚项不起作用，所以$\hat{g}_1$和$\hat{g}_2$等价。



